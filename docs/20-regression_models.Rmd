# Bayesian applied regression models {#day2}

## Posterior predictive checks
Posterior predictive check is a fundamental step of the Bayesia
workflow to check if the fitted model makes sense. It basically
works by simulating replicated data from the fitted model and by 
comparing them to the observed data. 
Once the simulation has been performed, the following question
should arise: 

* Are the simulated data plausible with the real data? 
* Does our model make sense with our situation?

Simulating fake data from the fitted means that we must draw values
from the posterior distribution of unknown but observable outcome data,
i.e.

\begin{equation}

p \left( \widetilde{y} \vert y \right) = \int p \left( \widetilde{y} 
\vert \theta \right) p \left( \theta  \vert y \right) d \theta

\end{equation}

Drawing from the posterior predictive distribution can be implemented
as follows:

* Draw $\theta^{*}$ from the posterior 
  $p \left( \theta  \vert y \right)$
  
* Draw $y^{*}$ from the posterior predictive distribution
  $p \left( y  \vert \theta^{*} \right)$

If there are differences between the posterior predictive distribution
and the empirical distribution of the observed data it means that
the model isn't able of describing our data and it is probably
misfitting or overfitting. How can we inspect these discrepancies?
  
* Graphical posterior predictive checks are a valuable tool to assess 
  discrepancies between simulated data and real data.
    
* We can also compare some observed statistics and posterior predictive
  statistics, e.g. the mean of observed data and the distribution of 
  mean from the data simulated with our model.

* In posterior predictive distribution we have many sources of
uncertainty: about the parameters, given by the posterior distribution
and about the data, given by their variation and their noise.

_In posterior predictive check  is used to see if the simulated data range among plausible value. Thus, we don't want that the simulated
data perfectly resemble the observed data, otherwise our model we
are running into some overfitting problems._

Let's come back to the case study of the pest control of apartment
buildings. We are going to perform some posterior predictive checks to
understand the implications of our simple poisson regression.

To simulate replicated data from our model we need to add the
_generated quantities_ block in the Stan program.

```{stan, output.var = "code_example_simple_poisson_gen_quant"}
/* Simple Poisson model*/

/*
* Alternative to poisson_log_rng() that
* avoids potential numerical problems during warmup
*/
functions {
  int poisson_log_safe_rng(real eta) {
    real pois_rate = exp(eta);
    if (pois_rate >= exp(20.79))
      return -9;
    return poisson_rng(pois_rate);
  }
}

// Declare the structure of the data
data {
  int<lower = 1> N;                     // number of observation
  vector<lower = 0>[N] n_traps;         // number of traps
  int<lower = 0> complaints[N];         // number of complaints
}

// Declare the parameters of the model (the goal of our inferemce)
parameters {
  // Intercept and the slope of the linear predictor eta
  real alpha;
  real beta;
}

// Define the structure of the Poisson model: priors and likelihood
model {
  // Let's create the linear predictor: eta
  vector[N] eta = alpha + beta * n_traps;

  // likelihood
  target += poisson_log_lpmf(complaints | eta);

  // Priors on the parameters
  target += normal_lpdf(alpha | log(4), 1) +
            normal_lpdf(beta | -0.25, 1);
}

// Predictive posterior distributions in generated quantities
generated quantities {

  int y_rep[N];       // replicated y as array with vector of integers

  // Simulate data from the fitted modedl
  for (n in 1:N) {

    real eta_n = alpha + beta * n_traps[n];   // linear predictor
    y_rep[n] = poisson_log_safe_rng(eta_n);   // replicated complaints

  }
}
```


Now we generate posterior predictive values from our model and store
them into a matrix.

```{r}
# A seed for reproducibility
mcmc_seed <- 140509

simple_poisson_gen_quant <- stan(
  file = "stan_programs/simple_poisson_gen_quant.stan",
  data = pest_data_list,
  chains = 1L,
  warmup = 1000L,
  iter = 2000L,
  seed = mcmc_seed
)

# Get replicated data into a matrix
y_rep_simple_poisson <- as.matrix(
  simple_poisson_gen_quant,
  pars = c("y_rep")
)

# Check the dimension
dim(y_rep_simple_poisson)
```

Each row corresponds to a distribution of posterior predicted values
given a combination of our parameters $\theta$ from the posterior
$p \left( \theta \vert y \right)$. Each column corresponds to a
posterior predictive distribution of replicated data for one 
observation in the dataset. Thus, the posterior predictive 
distribution is like a distribution of datasets.

Let's plot some draws from the posterior predictive distribution and
compare it with the observed data.

```{r}
# Posterior predictive checks
ppc_dens_overlay(
  y = pest_data$complaints, 
  y_rep_simple_poisson[1:200,]
)
```

As we can see, simulated data from our model do not resemble very well
the distribution of our model.

We can also compare some statistics. Given our example, zero complaints
is a value of interest for the manager of the company, because it
means no complaints. We can compare the proportion of zeros (no
complaints) in the observed data with the distribution of zeros from
the posterior predictive distribution.

```{r}
# Function that computes the proportion of zeros
prop_zero_fun <- function(x) mean(x == 0)

# Compare the proportions
ppc_stat(
  y = pest_data$complaints,
  yrep = y_rep_simple_poisson[1:200,],
  stat = "prop_zero_fun"
)
```

We can also plot the stanrdized values of the differences between
the observed number of complaints and the predicted number of
complaints (residuals).

```{r}
mean_y_rep <- colMeans(y_rep_simple_poisson)

std_resid <- (pest_data$complaints - mean_y_rep) / sqrt(mean_y_rep)

qplot(mean_y_rep, std_resid) + 
  hline_at(2) + 
  hline_at(-2) +
  theme_bw()
```
 
There are more positive than negative residuals, suggesting that
our model tends to underestimate the number of complaints.
 
We can use the rootogram to compare the observed and the expected
number of complaints.

```{r}
# Comparison of observed and expected number of complaints
ppc_rootogram(
  pest_data$complaints,
  yrep = y_rep_simple_poisson
)
```

```{r}
ppc_intervals(
  y = pest_data$complaints, 
  yrep = y_rep_simple_poisson,
  x = pest_data$traps
) + 
  labs(
    x = "Number of traps", 
    y = "Number of complaints"
  )
```

Graphical posterior predictive checks inform us that the model may
not be a good choice for our problem at hand. We need to criticize
our model and understand if some of assumptions we made can be
revised. 

For example, apartments may be very different from
each other and it is better to incorporate this information in the
model. To see if that makes sense, we can plot the relationship between
the squared foot of the building and the number of complaints.

```{r}
ggplot(
  data = pest_data, 
  mapping = aes(
    x = log(total_sq_foot), 
    y = log1p(complaints)
  )
) + 
  geom_point() + 
  geom_smooth(
    method = "lm", 
    se = FALSE
  ) +
  theme_bw()
```

It seems that the bigger the apartment the greater the number of 
complaints. Maybe bigger building may be subjected
to a higher number of roaches, and thus more complaints. We can
think the size of the aparment as an initial exposure that likely
influnce the number of complaints and we can add it in the model as an
offset. Moreover, we can also take into account the level of the
apartment. Here we are going to rescale the square foot measure such 
that it is on a unit scale to make computation more feasible.

```{r}
pest_data$log_sq_foot <- log(pest_data$total_sq_foot/1e4)
```

Following the steps of the Bayesian workflow, we start by simulating
fake data from the prior predictive distribution. We can do it in Stan
by writing in the program the number of simulated observation in the 
data block and declaring the priors and the simulated data in the
generated quantities block.

```{stan, output.var = "code_example_multi_poisson_dgp"}
/* Generating data from a multiple Poisson regression model*/

/*
* Alternative to poisson_log_rng() that
* avoids potential numerical problems during warmup
*/
functions {
  int poisson_log_safe_rng(real eta) {
    real pois_rate = exp(eta);
    if (pois_rate >= exp(20.79))
      return -9;
    return poisson_rng(pois_rate);
  }
}

data {
  // Number of observations
  int<lower=1> N;
}

model{

}

// Predictive posterior distributions in generated quantities
generated quantities {

  // Declare simulated data
  vector[N] log_sq_foot;
  int live_in_super[N];
  int n_traps[N];
  int complaints[N];

  // Generate parameters values from the prior predictive distribution
  real alpha = normal_rng(log(4), 1);
  real beta = normal_rng(-0.25, 1);
  real beta_super = normal_rng(-0.5, 1);

  // Generate simulated values of the outcome (number of complaints)
  for(n in 1:N) {

    // Generate fake data
    log_sq_foot[n] = normal_rng(1.5, 1);
    live_in_super[n] = bernoulli_rng(0.5);
    n_traps[n] = poisson_rng(8);

    // Generate simulated number of complaints
    complaints[n] = poisson_log_safe_rng(alpha +
                    log_sq_foot[n] + beta * n_traps[n] +
                    beta_super * live_in_super[n]);
  }
}

```

We now simulate fake data from the model.

```{r}
# Compile the model
fake_pest_model <- stan_model(
  file = "stan_programs/multiple_poisson_dgp.stan"
)

# Sampling from the posterior
fake_pest_sampling <- sampling(
  fake_pest_model,
  data = list(N = nrow(pest_data)),
  chains = 1,
  cores = 1,
  iter = 1,
  algorithm = 'Fixed_param',
  seed = 123
)

# Get the fake data
fake_data_pest <- rstan::extract(fake_pest_sampling)
```


### Negative Binomial model
We may criticize the assumption of our model: constant mean and 
variance across observations. We can use a negative binomial 
distribution to model the number of complaints. The model can be
formalized as follows:

$$
\begin{align*}
\text{complaints}_{b,t} & \sim \text{Neg-Binomial}(\lambda_{b,t}, \phi) \\
\lambda_{b,t} & = \exp{(\eta_{b,t})} \\
\eta_{b,t} &= \alpha + \beta \, {\rm traps}_{b,t} + \beta_{\rm super} 
\, {\rm super}_{b} + \text{log_sq_foot}_{b}
\end{align*}
$$

In Stan the negative binomial mass function we'll use is called 
$\texttt{neg_binomial_2_log}(\text{ints} \, y, \text{reals} \, \eta, \text{reals} \, \phi)$ in Stan. Like the `poisson_log` function, this
negative binomial mass function that is parameterized in terms of its
log-mean, $\eta$, but it also has a precision $\phi$ such that

\begin{equation}
\mathbb{E}[y] \, = \lambda = \exp(\eta)
\end{equation}

\begin{equation}
\text{Var}[y] = \lambda + \lambda^2/\phi = \exp(\eta) + \exp(\eta)^2 /
\phi.
\end{equation}

As $\phi$ gets larger the term $\lambda^2 / \phi$ approaches zero and 
so the variance of the negative-binomial approaches $\lambda$, i.e., 
the negative-binomial gets closer and closer to the Poisson.

## MCMC algorithms
Once we have very complex model with high number of parameters it is
very difficult to get an analytic approach to sample from the
posterior distribution. 
In such case, the posterior may be very complex and sampling from it
becomes very complicated.

Regarding our pest control examples, suppose we want to allow for
different intercepts across buildings, or different slopes. Thus, the 
dimensionality of the parameter space becomes high and doing 
numerical computation in such domains is very difficult. We need to
find something that approximate the "surface" of the posterior 
distribution of the parameter space, something able to explore the
most important region of such distribution. 

How can we do when we have a very strange and complicated surface that
we don't know how it looks like?

Markov Chain Monte Carlo (MCMC) is useful tool to perform such
exploration.


Diagnose the sampling process of the algorithm is crucial to 
understand if we can draw good inference.


