[
["index.html", "An Introduction to Bayesian Data Analysis with Stan Information on the course Settings", " An Introduction to Bayesian Data Analysis with Stan Daniele Bottigliengo 2018-09-17 Information on the course This book contains notes and exercises from the course on Bayesian Data Analysis with Stan taken during the 1st International Summer School at the Department of Brain and Behavioral Sciences Medical Statistics and Genomic Unit, University of Pavia, Italy. The course was taught by Jonah Gabry, from Columbia University, and Leonardo Egidi, from University of Trieste. Settings Here, there are the libraries loaded during the course, w/ the relative options, plus some packages and options useful to write code more understandable by humans obtaining nicer output. # Packages for the analyses library(rstan) # R interface to Stan # For execution on a local, multicore CPU with excess RAM options(mc.cores = parallel::detectCores() - 1) # To avoid recompilation of unchanged Stan programs rstan_options(auto_write = TRUE) # Package for Bayesian inference visualization library(bayesplot) # Package for model comparison library(loo) # Package(s) for data management library(tidyverse) # Imports the principal tidyverse packages # Document output options knitr::opts_chunk$set( comment = &#39;#&gt;&#39;, echo = TRUE, # Render all the code message = FALSE, # Do net render messages warning = FALSE, # Do not render warnings fig.height = 4.4, # height to allow two figures in a PDF page cache.extra = knitr::rand_seed # cache seeds to assure reproducibility ) The following code create the packages.bib files which is the BibTeX lists of all the packages references we have loaded. # Automatically create a bib database for the loaded packages knitr::write_bib( c( .packages(), &#39;bookdown&#39;, &#39;knitr&#39;, &#39;rmarkdown&#39;, &#39;knitr&#39;, &#39;tidyverse&#39;, &#39;rstan&#39;, &quot;bayesplot&quot; ), file = &#39;packages.bib&#39; ) "],
["day1.html", "Day 1 Foundations of Bayesian inference in theory and practice and Stan software 1.1 What is Stan? 1.2 Bayesian workflow 1.3 World concentration ofo PM2.5: case study 1.4 Pest control of roaches in apartment buildings: a case study", " Day 1 Foundations of Bayesian inference in theory and practice and Stan software 1.1 What is Stan? Open source probabilistic programming language How does Stan work? declare data and the parameters of the model who you want to fit to the phenomenon under study define a posterior distribution for the parameters or a penalized likelihood 1.2 Bayesian workflow Bayesian data analysis workflow follows the following steps: Exploratory data analysis: look at the data to understand the variation of the phenomenon and to think about what model should fit the data Prior predictive checking: simulate fake data that look like real data drawing values of the parameters from the prior predictive distribution and fit the model to the fake data Check implications of the fitted model and how different prior distributions impact inference Understand the fitted model (if it makes sense in our particular domain) Fit the model to the data Diagnostics the algorithm: check if the algorithm used to sample from the posterior distribution showed some pathological behaviours that affected the sampling and thus the final inference Posterior predictive checking Simulate from the fitted model outcome values and evaluate them with observed data. Does the model fit well the data? Compare different fitted models to understand which model best represents your data (for example using cross-validation) 1.3 World concentration ofo PM2.5: case study Goal: estimate global PM2.5 concentration for different regions of the world Problem: most of the data come from noisy satellite measurements (ground monitor provides sparse, heterogeneous coverage) 1.3.1 Exploratory Data Analysis: building a network of model How to define regions where concentrations may vary? WHO Regions Regions defined by clustering of the concentration levels It is possible to start visualizing and fiting different linear models for different regions identified by the clusters to understand how PM2.5 concentrations vary for different satellite measurements and if the variation is different across regions. For some regions there’s a lot of variation and low number of measurements. Inference from the region with noisy data may be unreliable if the region is considered completely separated from the others. Partial Pooling: bring information from different groups ( group with more information) to inform about the concentration of other regions (the ones with noisy data). Don’t treat the regions as completely different or as they are the same. Model 1: simple linear regression assuming all regions are equal Outcome: log(PM2.5) Covariate: satellite measurements (sat) Measurements: \\(n = 1, ..., N\\) Regions: \\(j = 1, ..., J\\) \\[\\begin{equation} \\log \\left( PM2.5 \\right) \\sim N \\left( \\alpha + \\beta \\log \\left( sat_{nj} \\right), \\sigma \\right) \\end{equation}\\] Model 2: \\[\\begin{equation} \\log \\left( PM_{2.5, nj} \\right) \\sim N \\left( \\mu_{nj}, \\sigma \\right) \\\\ \\\\ \\mu_{nj} = \\alpha_{0} + \\alpha_{j} + \\left( \\beta_{0} + \\beta_{j} \\right) \\log \\left( sat_{nj} \\right) \\end{equation}\\] Priors on region-levels parameters: \\[\\begin{equation} \\alpha_{j} \\sim N \\left( 0, \\tau_{\\alpha} \\right) \\\\ \\beta_{j} \\sim N \\left( 0, \\tau_{\\beta} \\right) \\end{equation}\\] We impose probability distribution on the region-level paramters, to allow for different interecepts and slopes for different regions We are evaluating the possibility that PM2.5. concentration varies, along with satellite measurements, differently from region to region. Priors are centered around 0 region’s intercepts vary around the constant intercept \\(\\alpha_{0}\\). 1.3.2 Prior Predictive Checks: fake data can be as valuable as real data If it is harder to get intuition on the choice of the model, simulating fake data from the model can help to check which model gives the most plausible data distribution. Build a Bayesian model means defining a joint model for the data, expressed as y, and the unknowns (parameters and future data), expressed as \\(\\theta\\). Given the priors belief and the data that we observed, we want to know the posterior distribution of parameters \\(\\theta\\) to make inference- In Bayesian statistics there is no distinction between a missing data and a parameter. The main distinction is between the observed, i.e. the data, and the unobserved, i.e. parameters or missing data, which is everything that is unknow. Problems with vague priors ! The problem with the use of improper priors is that we don’t specify a joint model of the data and the parameters We don’t specify a data generating process We don’t regularize infereces We give to much probability to parameter values that are implausible (large values that approach infinty) Proper and vague priors are better, but still problematic Examples of traditional prior distributions: \\[\\begin{equation} \\alpha_{0} \\sim N \\left( 0, 100 \\right) \\\\ \\beta_{0} \\sim N \\left( 0, 100 \\right) \\\\ \\tau_{\\alpha} \\sim InvGamma \\left( 1, 100 \\right) \\\\ \\tau_{\\beta} \\sim InvGamma \\left( 1, 100 \\right) \\end{equation}\\] How we build a generative model? Simulate a value \\(\\theta^{*}\\) from the prior predictive distribution \\(p \\left( \\theta \\right)\\). Plug the value into the model and simulate \\(y^{*} \\sim p \\left( y \\vert \\theta \\right)\\) If data are simulated with these priors you get very unlikely values (physically impossible) \\(\\rightarrow\\) PM2.5 concentrations values that range between 200 and 800 on the log scale! Unless data are very informative, you get bad inference What are better priors for the problem at hand? \\[\\begin{equation} \\alpha_{0} \\sim N \\left( 0, 1 \\right) \\\\ \\beta_{0} \\sim N \\left( 1, 1 \\right) \\\\ \\tau_{\\alpha} \\sim N^{+} \\left( 0, 1 \\right) \\\\ \\tau_{\\beta} \\sim N^{+} \\left( 0, 1 \\right) \\end{equation}\\] Simulated data cover a range of more plausible values given the problem at hand Weakly Informative Prios: they work pretty well with sparse and noisy data 1.4 Pest control of roaches in apartment buildings: a case study If you live in an apartment building there may a cost to deal with roaches infestation. Suppose you are a statistician hired by the company who owns sevaral apartment building in a city to answer to the following question: how much should we spend to balance the cost of the pest control and to balance the complaints of the people living in the apartments? 1.4.1 The goal The decision problem can be formalized as follows: \\[\\begin{equation} \\arg\\max_{\\textrm{traps} \\in \\mathbb{N}} \\mathbb{E}_{\\text{complaints}}[R(\\textrm{complaints}(\\textrm{traps})) - \\textrm{TC}(\\textrm{traps})] \\end{equation}\\] Basically the manager of the company wants to understand which is number of traps (traps) that maximizes the balance between lost revenue (R) generated by resident’s complaints total cost (TC) of maintaining the traps. Both complaints and total costs are function of the number of traps. From a Bayesian perspective, the problem can be thought at 2 stages: * Fitting a model to the data and draw values of the parameters from their posterior distributions. * Make decision based on the inference we draw using the posterior distribution of the parameter For every problem we face, we don’t need to be afraid to used complex models with lot of parameters. Complicated models that reflect our uncertainty in the problem at hand are needed to correctly support decision-making process . Let’s now load the data and look at the variables. pest_data &lt;- readRDS( here::here( &quot;data/pest_data.RDS&quot; ) ) str(pest_data) #&gt; &#39;data.frame&#39;: 120 obs. of 14 variables: #&gt; $ mus : num 0.369 0.359 0.282 0.129 0.452 ... #&gt; $ building_id : int 37 37 37 37 37 37 37 37 37 37 ... #&gt; $ wk_ind : int 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ date : Date, format: &quot;2017-01-15&quot; &quot;2017-02-14&quot; ... #&gt; $ traps : num 8 8 9 10 11 11 10 10 9 9 ... #&gt; $ floors : num 8 8 8 8 8 8 8 8 8 8 ... #&gt; $ sq_footage_p_floor : num 5149 5149 5149 5149 5149 ... #&gt; $ live_in_super : num 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ monthly_average_rent: num 3847 3847 3847 3847 3847 ... #&gt; $ average_tenant_age : num 53.9 53.9 53.9 53.9 53.9 ... #&gt; $ age_of_building : num 47 47 47 47 47 47 47 47 47 47 ... #&gt; $ total_sq_foot : num 41192 41192 41192 41192 41192 ... #&gt; $ month : num 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ complaints : num 1 3 0 1 0 0 4 3 2 2 ... 1.4.2 Exploratory data analysis Let’s do some plots of the data as exploratory data analysis. Let’s look at the number of complaints and how they vary ggplot( data = pest_data, mapping = aes( x = complaints ) ) + geom_bar() + theme_bw() Let’s plot the number of complaints for different buildings across the months during which traps were settled. Colors refer to apartment in different levels of the building. ggplot( data = pest_data, aes( x = date, y = complaints, color = live_in_super == TRUE ) ) + geom_line( aes(linetype = &quot;Number of complaints&quot;) ) + geom_point(color = &quot;black&quot;) + geom_line( aes( y = traps, linetype = &quot;Number of traps&quot; ), color = &quot;black&quot;, size = 0.25 ) + facet_wrap( ~building_id, scales = &quot;free&quot;, ncol = 2, labeller = label_both ) + scale_x_date( name = &quot;Month&quot;, date_labels = &quot;%b&quot; ) + scale_y_continuous( name = &quot;&quot;, limits = range(pest_data$complaints) ) + scale_linetype_discrete(name = &quot;&quot;) + scale_color_discrete(name = &quot;Live-in super&quot;) + theme_bw() We can see that there’s lot of variation in the number of complaints both across bulding and level of the apartment in the building. How many zeros are present in the data? # Number of zeros sum(pest_data$complaints == TRUE) #&gt; [1] 22 What’s their percentage? # Percentage of zeros sum(pest_data$complaints == TRUE)/nrow(pest_data) #&gt; [1] 0.1833333 1.4.3 The model We will start with a simple model and then we will add structure to the model to get more realistic answers. We will further deal with the apartments as if they are not similar nor completely different. For now, we are going to model the number of complaints with a simple Poisson model using the number of traps as the only covariate. The model can be formalized as follows \\[ \\begin{align*} \\textrm{complaints}_{b,t} &amp; \\sim \\textrm{Poisson}(\\lambda_{b,t}) \\\\ \\lambda_{b,t} &amp; = \\exp{(\\eta_{b,t})} \\\\ \\eta_{b,t} &amp;= \\alpha + \\beta \\, \\textrm{traps}_{b,t} \\end{align*} \\] Number of complaints are modeled as count data with Poisson regression, where \\(b\\) is the number of apartment and \\(t\\) is the month. As we mentioned above, we will start by assuming constant mean and variance across buildings, which is a very restrictive assumption. Let’s now fit the model. First we need to pass the data to the stan object as a list, declaring the number of observation, the vector of the outcome (number of complaints) and the vector (or also the matrix) of the covariate (number of traps): # Data as list to pass to stan file pest_data_list &lt;- list( N = nrow(pest_data), complaints = pest_data[[&quot;complaints&quot;]], n_traps = pest_data[[&quot;traps&quot;]] ) How is the Stan program that fit the model written? Stan programs are structured as “blocks”. In each block the user build a piece of the model that will be fitted to the data. The core blocks of a Stan program are the following: A data block: the user specify the data that will be used to fit the model. A parameters block: the user specify the parameters of the model that she wants to fit to the data. A model block: the user specify the structure of the model in terms of priors distributions for the unobserved parameters and likelihood for the observed data. A generated quantities block: the user specificy the quantities of interest that she wants to simulated given the posterior distribution of the model (predictive or future values, missing data, ecc.) Let’s start to write the Stan program of our model. data { int&lt;lower = 1&gt; N; // number of observation vector&lt;lower = 0&gt;[N] n_traps; // number of traps int&lt;lower = 0&gt; complaints[N]; // number of complaints } parameters { // Intercept and the slope of the linear predictor eta real alpha; real beta; } model { // Let&#39;s create the linear predictor: eta vector[N] eta = alpha + beta * n_traps; /*Let&#39;s declare our outcome variable and its pdf. Poisson_log function directly exponentiated the linear prediction*/ target += poisson_log_lpmf(complaints | eta); /*or equivalently complaints ~ poisson_log(eta); */ /*Let&#39;s declare our priors distributions. Let&#39;s put some reasonable priors. In particular, we expect that for higher number of traps there will be less complaints from people living in the building.*/ target += normal_lpdf(alpha | log(4), 1) + normal_lpdf(beta | -0.25, 1); /*or equivalently alpha ~ normal(log(4), 1); beta ~ normal(-0.25, 1); */ } 1.4.3.1 Data block As we can see, we declared the number of observations of our dataset as an integer constrained to be equal to or greater than \\(1\\) (the program won’t work if the number of observations is 0 or negative, and that make sense because those value mean no data at hand or they would be non-senso.), whereas the number of traps is expressed as a vector of lenght N whouse values are constrained to be strictly positive (and that makes sense because a negative values won’t make any sense). The number of complaints has a curious expression. It is represented as an array that contains a vector of lenght equal to N filled with strictly positive integers, i.e. the numbere of complaints (which again cannot take a negative value). In Stan arrays are essentially “boxes” that contains objects (real numbers, vectors, matrices and so on). The elements of the objects must be of the same type. Here wr are using array syntax because the numbers of complaints are integers and Stan allows for vectors and matrices of only real numbers. 1.4.3.2 Parameters block The parameters of the model are declared as real numbers and they represent the intercept and the slope of the linear predictor \\(\\eta\\) of our model. 1.4.3.3 Model block First we expressed the linear predictor \\(\\eta\\) given by the product of a vector (the number of traps) and a scalar (the slope) plus a vector (the intercept). After that we define the likelihood of the data and the priors of the parameters. Both the likelihood and the priors will feed the objective of our inference, that here is expressed as “target”. Regarding the likelihood (first part of the target), we are telling to Stan to start by evaluating the log-likelihood (working with the log-likelihood is more computational stable) at an initial value (usually zero). After that initial evaluation, the target will be fed by the sum (not the product because we are working on the logarithm of the likelihood) of the contribution to the complete likelihood of each observation in the data. Regarding the priors, the reasoning is the same. Their contribution to the target is given by the values of the parameters drawn from the distribution that will affect eta, thus the impact of the observations on the likelihood. Thus, we can think to the target as something which is defined by initial evaluation of itself to a particular value and that is continuously fed by the observed data, through the likelihood, and the prior distributions. Likelihood and priors can also be expressed in a more familiar form, such as the one expressed in @ref(eq:1.7)) and given in the comment blocks. For now we stop here. We will see in the next lessons how to simulated data from our model using the generated quantities block. Now let’s compile the model and sample from the posterior distribution of the parameters. To do that, we need to save the Stan program into a separate file and call it into the stan function. # A seed for reproducibility of the draws from the posterior mcmc_seed &lt;- 140509 # Compile the model in C++ simple_poisson_comp &lt;- stan_model( file = &quot;stan_programs/simple_poisson.stan&quot; ) simple_poisson &lt;- sampling( simple_poisson_comp, data = pest_data_list, chains = 1L, # One chain for a quick sampling seed = mcmc_seed ) #&gt; #&gt; SAMPLING FOR MODEL &#39;simple_poisson&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 0 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.256 seconds (Warm-up) #&gt; 0.285 seconds (Sampling) #&gt; 0.541 seconds (Total) Let’s look at the posterior of the parameters. simple_poisson #&gt; Inference for Stan model: simple_poisson. #&gt; 1 chains, each with iter=2000; warmup=1000; thin=1; #&gt; post-warmup draws per chain=1000, total post-warmup draws=1000. #&gt; #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff #&gt; alpha 2.57 0.01 0.16 2.24 2.47 2.58 2.69 2.86 232 #&gt; beta -0.19 0.00 0.02 -0.24 -0.21 -0.19 -0.18 -0.14 221 #&gt; lp__ -351.03 0.06 1.01 -353.60 -351.49 -350.72 -350.30 -350.02 302 #&gt; Rhat #&gt; alpha 1 #&gt; beta 1 #&gt; lp__ 1 #&gt; #&gt; Samples were drawn using NUTS(diag_e) at Mon Sep 17 20:12:01 2018. #&gt; For each parameter, n_eff is a crude measure of effective sample size, #&gt; and Rhat is the potential scale reduction factor on split chains (at #&gt; convergence, Rhat=1). These are summaries of the posterior distributions: mean, error associated to MCMC, standard deviation, quantiles. Let’s now plot histograms of the posterior of the parameters. # Store parameters alpha and beta into a matrix post_pars &lt;- as.matrix( simple_poisson, pars = c(&quot;alpha&quot;, &quot;beta&quot;) ) mcmc_hist(post_pars) Or the scatter plot of the bivariate distribution mcmc_scatter(post_pars) Our goal for inference is to understand what the posterior distribution of the parameters of the model implies for the outcome and its simulated data (predictive data) from the model. Nevertheless, we need to check the behavior of the sampler: a pathological behavior means that the algorithm did not sample correctly from the posterior. Thus, interpreation of such models may be misleading, leading to biased inferece. In the next lesson we will see how to diagnose the algorithm used to draw from the posterior distribution of the paramters. "],
["day2.html", "Day 2 Bayesian applied regression models 2.1 Posterior predictive checks 2.2 MCMC algorithms", " Day 2 Bayesian applied regression models 2.1 Posterior predictive checks Posterior predictive check is a fundamental step of the Bayesian workflow to check if the fitted model makes sense. It basically works by simulating replicated data from the fitted model and by comparing them to the observed data. Once the simulation has been performed, the following question should arise: Are the simulated data plausible with the real data? Does our model make sense with our situation? Simulating fake data from the fitted model means that we must draw values from the posterior distribution of unknown but observable outcome data, i.e. \\[\\begin{equation} p \\left( \\widetilde{y} \\vert y \\right) = \\int p \\left( \\widetilde{y} \\vert \\theta \\right) p \\left( \\theta \\vert y \\right) d \\theta \\end{equation}\\] Drawing from the posterior predictive distribution can be implemented as follows: Draw \\(\\theta^{*}\\) from the posterior \\(p \\left( \\theta \\vert y \\right)\\) Draw \\(y^{*}\\) from the posterior predictive distribution \\(p \\left( y \\vert \\theta^{*} \\right)\\) If there are differences between the posterior predictive distribution and the empirical distribution of the observed data it means that the model isn’t able to accurately describe our data and it is probably misfitting or overfitting. How can we inspect these discrepancies? Graphical posterior predictive checks are a valuable tool to assess discrepancies between simulated data and real data. We can also compare some observed statistics and posterior predictive statistics, e.g. the mean of observed data and the distribution of mean from the data simulated with our model. In posterior predictive distribution we have many sources of uncertainty: about the parameters, given by the posterior distribution, and about the data, given by their variation and their noise. Posterior predictive check is used to see if the simulated data range among plausible value. Thus, we don’t want that the simulated data perfectly resemble the observed data, otherwise our model is very likely overfitting. Let’s come back to the case study of the pest control of apartment buildings. We are going to perform some posterior predictive checks to understand the implications of our simple poisson regression. To simulate replicated data from our model we need to add the generated quantities block in the Stan program. /* Simple Poisson model generated quantities*/ /* * Alternative to poisson_log_rng() that * avoids potential numerical problems during warmup */ functions { int poisson_log_safe_rng(real eta) { real pois_rate = exp(eta); if (pois_rate &gt;= exp(20.79)) return -9; return poisson_rng(pois_rate); } } // Declare the structure of the data data { int&lt;lower = 1&gt; N; // number of observation vector&lt;lower = 0&gt;[N] n_traps; // number of traps int&lt;lower = 0&gt; complaints[N]; // number of complaints } // Declare the parameters of the model (the goal of our inferemce) parameters { // Intercept and the slope of the linear predictor eta real alpha; real beta; } // Define the structure of the Poisson model: priors and likelihood model { // Let&#39;s create the linear predictor: eta vector[N] eta = alpha + beta * n_traps; // likelihood target += poisson_log_lpmf(complaints | eta); // Priors on the parameters target += normal_lpdf(alpha | log(4), 1) + normal_lpdf(beta | -0.25, 1); } // Predictive posterior distributions in generated quantities generated quantities { int y_rep[N]; // replicated y as array with vector of integers // Simulate data from the fitted modedl for (n in 1:N) { real eta_n = alpha + beta * n_traps[n]; // linear predictor y_rep[n] = poisson_log_safe_rng(eta_n); // replicated complaints } } Now we generate posterior predictive values from our model and store them into a matrix. # Compile the model simple_poisson_gen_quant_comp &lt;- stan_model( file = &quot;stan_programs/simple_poisson_gen_quant.stan&quot; ) # Sampling from the posterior simple_poisson_gen_quant &lt;- sampling( simple_poisson_gen_quant_comp, data = pest_data_list, chains = 4L, warmup = 1000L, iter = 2000L, seed = mcmc_seed ) # Get replicated data into a matrix y_rep_simple_poisson &lt;- as.matrix( simple_poisson_gen_quant, pars = c(&quot;y_rep&quot;) ) # Check the dimension dim(y_rep_simple_poisson) #&gt; [1] 4000 120 Each row corresponds to a distribution of posterior predicted values given a combination of our parameters \\(\\theta\\) from the posterior \\(p \\left( \\theta \\vert y \\right)\\). Each column corresponds to a posterior predictive distribution of replicated data for one observation in the dataset. Thus, the posterior predictive distribution is like a distribution of datasets. Let’s plot some draws from the posterior predictive distribution and compare it with the observed data. # Posterior predictive checks ppc_dens_overlay( y = pest_data$complaints, y_rep_simple_poisson[1:200,] ) As we can see, simulated data from our model do not resemble very well the distribution of the observed data. We can also compare some statistics. Given our example, zero complaints is a value of interest for the manager of the company, because it means no complaints. We can compare the proportion of zeros (no complaints) in the observed data with the distribution of zeros from the posterior predictive distribution. # Function that computes the proportion of zeros prop_zero_fun &lt;- function(x) mean(x == 0) # Compare the proportions ppc_stat( y = pest_data$complaints, yrep = y_rep_simple_poisson[1:200,], stat = &quot;prop_zero_fun&quot; ) The graph above clearly shows that our model tends to underestimate the proportion of zero complaints. We can also plot the stanrdized values of the differences between the observed number of complaints and the predicted number of complaints (residuals). mean_y_rep &lt;- colMeans(y_rep_simple_poisson) std_resid &lt;- (pest_data$complaints - mean_y_rep) / sqrt(mean_y_rep) qplot(mean_y_rep, std_resid) + hline_at(2) + hline_at(-2) + theme_bw() There are more positive than negative residuals, suggesting that our model tends to underestimate the number of complaints. We can use the rootogram to compare the observed and the expected number of complaints. # Comparison of observed and expected number of complaints ppc_rootogram( pest_data$complaints, yrep = y_rep_simple_poisson ) If the model had a good fit, we would had observed similar expected and observed count. However, we can see that our model tends to overestimate when there is a medium number of complaints, whereas it underestimates when there are few or lager complaints. We can further inspect our model by comparing the observed and predicted number of complaints (with relative uncertainty) across the number of traps. ppc_intervals( y = pest_data$complaints, yrep = y_rep_simple_poisson, x = pest_data$traps ) + labs( x = &quot;Number of traps&quot;, y = &quot;Number of complaints&quot; ) We can observe that our model predicts very badly the extreme of the data, i.e. few and large number of complaints. Graphical posterior predictive checks inform us that the model may not be a good choice for our problem at hand. We need to criticize our model and understand if some of assumptions we made can be revised. For example, building may be very different from each other and it is better to incorporate this information in the model. To see if that makes sense, we can plot the relationship between the squared foot of the building and the number of complaints. ggplot( data = pest_data, mapping = aes( x = log(total_sq_foot), y = log1p(complaints) ) ) + geom_point() + geom_smooth( method = &quot;lm&quot;, se = FALSE ) + theme_bw() It seems that the bigger the apartment the greater the number of complaints. Maybe bigger building may be subjected to a higher number of roaches, and thus more complaints. We can think the size of the aparment as an initial exposure that likely influnce the number of complaints and we can add it in the model as an offset. Moreover, we can also take into account the level of the apartment in the building. Here we are going to rescale the square foot measure such that it is on a unit scale making computation more feasible. pest_data$log_sq_foot &lt;- log(pest_data$total_sq_foot/1e4) Following the steps of the Bayesian workflow, we start by simulating fake data from the prior predictive distribution. We can do it in Stan by writing in the program the number of simulated observation in the data block and declaring the priors and the simulated data in the generated quantities block. /* Generating data from a multiple Poisson regression model*/ /* * Alternative to poisson_log_rng() that * avoids potential numerical problems during warmup */ functions { int poisson_log_safe_rng(real eta) { real pois_rate = exp(eta); if (pois_rate &gt;= exp(20.79)) return -9; return poisson_rng(pois_rate); } } data { // Number of observations int&lt;lower=1&gt; N; } model{ } // Predictive posterior distributions in generated quantities generated quantities { // Declare simulated data vector[N] log_sq_foot; int live_in_super[N]; int n_traps[N]; int complaints[N]; // Generate parameters values from the prior predictive distribution real alpha = normal_rng(log(4), 0.1); real beta = normal_rng(-0.25, 0.1); real beta_super = normal_rng(-0.5, 0.1); // Generate simulated values of the outcome (number of complaints) for(n in 1:N) { // Generate fake data log_sq_foot[n] = normal_rng(1.5, 0.1); live_in_super[n] = bernoulli_rng(0.5); n_traps[n] = poisson_rng(8); // Generate simulated number of complaints complaints[n] = poisson_log_safe_rng(alpha + log_sq_foot[n] + beta * n_traps[n] + beta_super * live_in_super[n]); } } We now simulate fake data from the model. # Compile the model multi_poisson_dgp_comp &lt;- stan_model( &quot;stan_programs/multiple_poisson_dgp.stan&quot; ) # Sampling from the posterior multi_poisson_dgp &lt;- sampling( multi_poisson_dgp_comp, data = list(N = nrow(pest_data)), chains = 1, cores = 1, iter = 1, algorithm = &#39;Fixed_param&#39;, seed = 123 ) #&gt; #&gt; SAMPLING FOR MODEL &#39;multiple_poisson_dgp&#39; NOW (CHAIN 1). #&gt; Iteration: 1 / 1 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0 seconds (Warm-up) #&gt; 0 seconds (Sampling) #&gt; 0 seconds (Total) # Get the fake data fake_data_pest &lt;- rstan::extract(multi_poisson_dgp) Now we prepare the data into a list to be passed to Stan. # Take only numbers from the first iteration stan_fake_data &lt;- list( N = nrow(pest_data), log_sq_foot = fake_data_pest$log_sq_foot[1, ], live_in_super = fake_data_pest$live_in_super[1, ], n_traps = fake_data_pest$n_traps[1, ], complaints = fake_data_pest$complaints[1, ] ) And we fit our model to the fake data. If our model is able to recover the parameters the prior distribution of the parameters than it means that our model is reasonable and we can fit it to the real data. Below the Stan program of the model /* Multi Poisson model*/ /* * Alternative to poisson_log_rng() that * avoids potential numerical problems during warmup */ functions { int poisson_log_safe_rng(real eta) { real pois_rate = exp(eta); if (pois_rate &gt;= exp(20.79)) return -9; return poisson_rng(pois_rate); } } // Declare the structure of the data data { int&lt;lower = 1&gt; N; // number of observation vector&lt;lower = 0&gt;[N] n_traps; // number of traps int&lt;lower = 0&gt; complaints[N]; // number of complaints vector[N] log_sq_foot; // log of square foot building vector&lt;lower = 0, upper = 1&gt;[N] live_in_super; // level of the apartment } // Declare the parameters of the model (the goal of our inferemce) parameters { // Intercept and the slope of the linear predictor eta real alpha; real beta; real beta_super; } // Define the structure of the Poisson model: priors and likelihood model { // Let&#39;s create the linear predictor: eta vector[N] eta = alpha + beta * n_traps + beta_super * live_in_super + log_sq_foot; /*Let&#39;s declare our outcome variable and its pdf. Poisson_log function directly exponentiated the linear prediction*/ target += poisson_log_lpmf(complaints | eta); /*or equivalently complaints ~ poisson_log(eta); */ /*Let&#39;s declare our priors distributions. Let&#39;s put some reasonable priors. In particular, we expect that for higher number of traps there will be less complaints from people living in the building.*/ target += normal_lpdf(alpha | log(4), 1) + normal_lpdf(beta | -0.25, 1) + normal_lpdf(beta_super | -0.5, 1); /*or equivalently alpha ~ normal(log(4), 1); beta ~ normal(-0.25, 1); */ } // Simulate replicated data from the model generated quantities { int y_rep[N]; for (n in 1:N) { real eta_rep = alpha + beta * n_traps[n] + beta_super * live_in_super[n] + log_sq_foot[n]; y_rep[n] = poisson_log_safe_rng(eta_rep); } } # Compile the model multi_poisson_comp &lt;- stan_model( &quot;stan_programs/multiple_poisson_regression.stan&quot; ) # Sampling from the posterior multi_poisson_fake &lt;- sampling( object = multi_poisson_comp, data = stan_fake_data, chains = 4L, warmup = 1000L, iter = 2000L ) Does the model recover the parameters of the prior? post_alpha_betas &lt;- as.matrix( multi_poisson_fake, pars = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;beta_super&quot;) ) true_alpha_beta &lt;- c( fake_data_pest$alpha, fake_data_pest$beta, fake_data_pest$beta_super ) mcmc_recover_hist( x = post_alpha_betas, true = true_alpha_beta ) Yes! Let’s fit the model to the real data and do posterior predictive checks. # Prepare the data into a list pest_data_list_multi &lt;- list( N = nrow(pest_data), n_traps = pest_data$traps, log_sq_foot = sqrt(pest_data$total_sq_foot/1e4), # rescale to unit scale live_in_super = pest_data$live_in_super, complaints = pest_data$complaints ) # Sampling from the posterior multi_poisson &lt;- sampling( multi_poisson_comp, data = pest_data_list_multi, chains = 4L, warmup = 1000L, iter = 2000L, seed = mcmc_seed ) # Get replicated data y_rep &lt;- as.matrix( multi_poisson, pars = c(&quot;y_rep&quot;) ) # Posterior predictive checks ppc_dens_overlay( y = pest_data$complaints, y_rep[1:200,] ) # Function that computes the proportion of zeros prop_zero_fun &lt;- function(x) mean(x == 0) # Compare the proportions ppc_stat( y = pest_data$complaints, yrep = y_rep[1:200,], stat = &quot;prop_zero_fun&quot; ) ppc_intervals( y = pest_data$complaints, yrep = y_rep, x = pest_data$traps ) + labs( x = &quot;Number of traps&quot;, y = &quot;Number of complaints&quot; ) The model still has some problems, in particular at the tails of the data: there is still an high number of complaints that our model is not able to account for. 2.1.1 Negative Binomial model We may criticize the assumption of our model: constant mean and variance across observations. We can use a Negative Binomial (NB) distribution to model the number of complaints allowing for overdispersion, i.e. variance not constant across observations. The model can be formalized as follows: \\[ \\begin{align*} \\text{complaints}_{b,t} &amp; \\sim \\text{Neg-Binomial}(\\lambda_{b,t}, \\phi) \\\\ \\lambda_{b,t} &amp; = \\exp{(\\eta_{b,t})} \\\\ \\eta_{b,t} &amp;= \\alpha + \\beta \\, {\\rm traps}_{b,t} + \\beta_{\\rm super} \\, {\\rm super}_{b} + \\text{log_sq_foot}_{b} \\end{align*} \\] We can write the NB function in Stan as \\(\\texttt{neg_binomial_2_log}(\\text{ints} \\, y, \\text{reals} \\, \\eta, \\text{reals} \\, \\phi)\\) in Stan. Like the poisson_log function, this negative binomial mass function that is parameterized in terms of its log-mean, \\(\\eta\\), but it also has a precision \\(\\phi\\) such that \\[\\begin{equation} \\mathbb{E}[y] \\, = \\lambda = \\exp(\\eta) \\end{equation}\\] \\[\\begin{equation} \\text{Var}[y] = \\lambda + \\lambda^2/\\phi = \\exp(\\eta) + \\exp(\\eta)^2 / \\phi. \\end{equation}\\] As \\(\\phi\\) gets larger the term \\(\\lambda^2 / \\phi\\) approaches zero and so the variance of the negative-binomial approaches \\(\\lambda\\), i.e., the NB gets closer and closer to the Poisson. 2.2 MCMC algorithms Once we have very complex model with high number of parameters it is very difficult to get an analytic approach to sample from the posterior distribution. In such case, the posterior may be very complex and sampling from it becomes very complicated. Regarding our pest control examples, suppose we want to allow for different intercepts across buildings, or different slopes. Thus, the dimensionality of the parameter space becomes high and doing numerical computation in such domains is very difficult. We need to find something that approximate the “surface” of the posterior distribution of the parameter space, something able to explore the most important region of such distribution. How can we do when we have a very strange and complicated surface that we don’t know how it looks like? Markov Chain Monte Carlo (MCMC) algorithms are useful tool to deal with such task. Stan samples from the posterior using the Hamiltonian Monte Carlo (HMC) algorithm, in particular the efficient No-U Turn Sampler (NUTS). For further details on the algorithm, we highly suggest the following readings: Hoffman and Gelman (2014) and Betancourt (2018). Here is a very nice example of how HMC works. The algorithm may incurs in some problems during the sampling process. If the sampler was not able to explore the surface of the posterior, then we get biased inference. Thus, it is very important to diagnose the sampling process of the algorithm to look for potential patological behaviors that could interfer our final goal. In the next lesson we will see how to build hierarchical models and how to diagnose the patological behaviours of the algorithm during the sampling. "],
["day3.html", "Day 3 Hierarchical/Multilevel modeling (part 1) 3.1 Pest control example: negative-binomial model 3.2 Pest control example: hierarchical model (varying intercept)", " Day 3 Hierarchical/Multilevel modeling (part 1) 3.1 Pest control example: negative-binomial model We now try to fit a NB to the data. We will start by simulating fake data from the prior predictive distribution. The Stan program to generate fake data is written as follows: // Multi NB data generating process functions { /* * Alternative to neg_binomial_2_log_rng() that * avoids potential numerical problems during warmup */ int neg_binomial_2_log_safe_rng(real eta, real phi) { real gamma_rate = gamma_rng(phi, phi / exp(eta)); if (gamma_rate &gt;= exp(20.79)) return -9; return poisson_rng(gamma_rate); } } data { int&lt;lower = 1&gt; N; // Number of observations } parameters { } model { } generated quantities { // Declare simulated variables vector[N] log_sq_foot; int live_in_super[N]; int traps[N]; int complaints[N]; // Generate parameter values from the prior predictive distribution real alpha = normal_rng(log(4), 0.1); real beta = normal_rng(-0.25, 0.1); real beta_super = normal_rng(-0.5, 0.1); real inv_phi = fabs(normal_rng(0, 1)); // Generate fake data for(n in 1:N) { // Generate covariates log_sq_foot[n] = normal_rng(1.5, 0.1); live_in_super[n] = bernoulli_rng(0.5); traps[n] = poisson_rng(8); // Generate outcome complaints[n] = neg_binomial_2_log_safe_rng( alpha + beta * traps[n] + beta_super * live_in_super[n] + log_sq_foot[n], inv(inv_phi) ); } } # Compile the model multi_NB_comp_dgp &lt;- stan_model( file = &quot;stan_programs/multi_nb_dgp.stan&quot; ) # Sampling from the prior predictive distributions fitted_fake_data_pest_NB &lt;- sampling( object = multi_NB_comp_dgp, data = list(N = nrow(pest_data)), chains = 1, cores = 1, iter = 1, algorithm = &#39;Fixed_param&#39;, seed = 123 ) #&gt; #&gt; SAMPLING FOR MODEL &#39;multi_nb_dgp&#39; NOW (CHAIN 1). #&gt; Iteration: 1 / 1 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0 seconds (Warm-up) #&gt; 0 seconds (Sampling) #&gt; 0 seconds (Total) # Get the fake data fake_data_pest_NB &lt;- rstan::extract(fitted_fake_data_pest_NB) Let’s now write the Stan program of the model we want to fit. // Multiple NB regression functions { /* * Alternative to neg_binomial_2_log_rng() that * avoids potential numerical problems during warmup */ int neg_binomial_2_log_safe_rng(real eta, real phi) { real gamma_rate = gamma_rng(phi, phi / exp(eta)); if (gamma_rate &gt;= exp(20.79)) return -9; return poisson_rng(gamma_rate); } } data { int&lt;lower = 1&gt; N; // number of observations vector[N] log_sq_foot; // log square foot of the building vector&lt;lower = 0, upper = 1&gt;[N] live_in_super; // is the bulding in super? vector&lt;lower = 0&gt;[N] traps; // number of traps in the building int&lt;lower = 0&gt; complaints[N]; // number of complaints in the building } parameters { real alpha; // intercept real beta; // coefficients on the traps real beta_super; // coefficients on live in super real&lt;lower = 0&gt; inv_phi; // inverse of phi coefficients } transformed parameters { real phi = inv(inv_phi); // phi coefficient } model { // Linear predictor vector[N] eta = alpha + beta * traps + beta_super * live_in_super + log_sq_foot; // Priors target += normal_lpdf(alpha | log(4), 1) + normal_lpdf(beta | -0.25, 1) + normal_lpdf(beta_super | -0.5, 1) + normal_lpdf(inv_phi | 0, 1); // Likelihood target += neg_binomial_2_log_lpmf(complaints| eta, phi); } generated quantities { // Declare simulated data from the model int y_rep[N]; for(n in 1:N) { real eta_rep = alpha + beta * traps[n] + beta_super * live_in_super[n] + log_sq_foot[n]; y_rep[n] = neg_binomial_2_log_safe_rng(eta_rep, phi); } } And we fit it to the fake data # Create a list to pass to Stan program fake_data_pest_NB_list &lt;- list( N = nrow(pest_data), log_sq_foot = fake_data_pest_NB$log_sq_foot[1, ], live_in_super = fake_data_pest_NB$live_in_super[1, ], traps = fake_data_pest_NB$traps[1, ], complaints = fake_data_pest_NB$complaints[1, ] ) # Compile the model multi_NB_reg_comp &lt;- stan_model( file = &quot;stan_programs/multi_nb_regression.stan&quot; ) # Sampling from the posterior fitted_model_NB_reg_fake &lt;- sampling( object = multi_NB_reg_comp, data = fake_data_pest_NB_list, warmup = 1000L, iter = 2000L, chains = 4L, seed = mcmc_seed ) post_alpha_betas &lt;- as.matrix( fitted_model_NB_reg_fake, pars = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;beta_super&quot;, &quot;inv_phi&quot;) ) true_alpha_beta &lt;- c( fake_data_pest_NB$alpha, fake_data_pest_NB$beta, fake_data_pest_NB$beta_super, fake_data_pest_NB$inv_phi ) mcmc_recover_hist( x = post_alpha_betas, true = true_alpha_beta ) The model recover the values of the parameters. Let’s now fit the model to the real data. # List of data to pass to the model pest_data_list_multi &lt;- list( N = nrow(pest_data), traps = pest_data$traps, complaints = pest_data$complaints, log_sq_foot = sqrt(pest_data$total_sq_foot/1e4), live_in_super = pest_data$live_in_super ) # Sampling from the posterior distribution fitted_model_NB_multi &lt;- sampling( object = multi_NB_reg_comp, data = pest_data_list_multi, warmup = 1000L, iter = 2000L, chains = 4L, seed = mcmc_seed ) Let’s now perform some posterior predictive checks. First we compare distribution of observed data with the distributions of replicated data from the fitted model # Get replicated data y_rep &lt;- as.matrix( fitted_model_NB_multi, pars = c(&quot;y_rep&quot;) ) # Compare the distributions ppc_dens_overlay( y = pest_data$complaints, yrep = y_rep[1:200, ] ) The proportion of zeros # Function that computes the proportion of zeros prop_zero_fun &lt;- function(x) mean(x == 0) # Compare the proportions ppc_stat( y = pest_data$complaints, yrep = y_rep[1:200,], stat = &quot;prop_zero_fun&quot; ) The observed number of complaints vs the expected number of complaints # Comparison of observed and expected number of complaints ppc_rootogram( pest_data$complaints, yrep = y_rep ) And the observed vs posterior predictive number of complaints given the number of traps. ppc_intervals( y = pest_data$complaints, yrep = y_rep, x = pest_data$traps ) + labs( x = &quot;Number of traps&quot;, y = &quot;Number of complaints&quot; ) The NB model had a better fit to the data than the poisson model. We have not fully considered the structure of our data, i.e. the observations are grouped into buildings. In such situations, it may be a good idea account for this, since the mean and the variance may not be constant for all observations (which are likely not i.i.d.). We can do posterior predictive checks comparing the observed and posterior predictive average number of complaints for each building. ppc_stat_grouped( y = pest_data$complaints, yrep = y_rep, group = pest_data$building_id, stat = &quot;mean&quot;, binwidth = 0.2 ) As we can see, our model was not able to capture different variability for some buildings. We can try to model the variation across buldings. 3.2 Pest control example: hierarchical model (varying intercept) We now relax the assumpionts of constant mean across all the buildings and allow for different different parameters (intercept or slope) across them. We can do this by fitting a hierarchical model, allowing the parameters of the model to vary by buildings. We can start by adding a varying intercept. The model can be formalized as follows: \\[ \\text{complaints}_{b,t} \\sim \\text{Neg-Binomial}(\\lambda_{b,t}, \\phi) \\\\ \\lambda_{b,t} = \\exp{(\\eta_{b,t})} \\\\ \\eta_{b,t} = \\mu_b + \\beta \\, {\\rm traps}_{b,t} + \\beta_{\\rm super}\\, {\\rm super}_b + \\text{log_sq_foot}_b \\\\ \\mu_b \\sim \\text{Normal}(\\alpha, \\sigma_{\\mu}) \\] In our Stan model, \\(\\mu_b\\) is the \\(b\\)-th element of the vector \\(\\texttt{mu}\\) which has one element per building. We have one predictor that varies only by building. Thus, we can rewrite the above model more efficiently like so: \\[ \\eta_{b,t} = \\mu_b + \\beta \\, {\\rm traps}_{b,t} + \\text{log_sq_foot}_b\\\\ \\mu_b \\sim \\text{Normal}(\\alpha + \\beta_{\\text{super}} \\, \\text{super}_b , \\sigma_{\\mu}) \\] Moreovere, we can exploit the fact that in our data we have information at the building level, i.e. the average age of the residents, the average age of the buildings, and the average per-apartment monthly rent. We can store this information into a matrix that we will call bulding data that has one row per bulding and four columns, one for each bulding-level predictor: live_in_super age_of_building average_tentant_age monthly_average_rent We can now write our model as follows: \\[ \\eta_{b,t} = \\alpha_b + \\beta \\, {\\rm traps} + \\text{log_sq_foot}\\\\ \\mu \\sim \\text{Normal}(\\alpha + \\texttt{building_data} \\, \\zeta, \\,\\sigma_{\\mu}) \\] 3.2.1 Preparing hierarchical data for Stan program We have to prepare the data to pass to the Stan program. N_months &lt;- length(unique(pest_data$date)) N_buildings &lt;- length(unique(pest_data$building_id)) # Add some IDs for building and month pest_data &lt;- pest_data %&gt;% mutate( building_fac = factor( building_id, levels = unique(building_id) ), building_idx = as.integer(building_fac), ids = rep( 1:N_months, N_buildings ), # Create month id (use it later with autoregressive models) mo_idx = lubridate::month(date) ) # Center and rescale the building specific data building_data &lt;- pest_data %&gt;% select( building_idx, live_in_super, age_of_building, total_sq_foot, average_tenant_age, monthly_average_rent ) %&gt;% # Select only one row per bulding (the other rows are identical) unique() %&gt;% # Reorder by building_id (not mandatory) arrange(building_idx) %&gt;% select(-building_idx) %&gt;% # Subtract the mean from each variable scale(scale=FALSE) %&gt;% as.data.frame() %&gt;% # Scale the variables such that they are roughly on unit scale mutate( # scale by constants age_of_building = age_of_building / 10, total_sq_foot = total_sq_foot / 10000, average_tenant_age = average_tenant_age / 10, monthly_average_rent = monthly_average_rent / 1000 ) %&gt;% as.matrix() # Make data list for Stan stan_dat_hier &lt;- with(pest_data, list(complaints = complaints, traps = traps, N = length(traps), J = N_buildings, M = N_months, log_sq_foot = log(pest_data$total_sq_foot/1e4), # Remove the log_sq_foot var (use it as exposure) building_data = building_data[, -3], mo_idx = as.integer(as.factor(date)), # K is the number of building-level predictors K = 4, building_idx = building_idx ) ) We rescaled the variables such that they are roughly on the same scale to make the computational process more easy. In fact, the sampling algorithm is faster when variables are roughly on the same scale and on the unit scale. 3.2.2 Centered parametrization Let’s see how the Stan program changes adding a varying intercept. // Hierarchical multiple Negative Binomial model functions { /* * Alternative to neg_binomial_2_log_rng() that * avoids potential numerical problems during warmup */ int neg_binomial_2_log_safe_rng(real eta, real phi) { real gamma_rate = gamma_rng(phi, phi / exp(eta)); if (gamma_rate &gt;= exp(20.79)) return -9; return poisson_rng(gamma_rate); } } data { int&lt;lower = 1&gt; N; int&lt;lower = 0&gt; complaints[N]; vector&lt;lower = 0&gt;[N] traps; vector[N] log_sq_foot; // Declare the hierarchical part int&lt;lower = 1&gt; J; // number of building int&lt;lower = 1&gt; K; // number of building variables matrix[J, K] building_data; // matrix of building data int&lt;lower = 1, upper = J&gt; building_idx[N]; // id of the building } parameters { real&lt;lower = 0&gt; inv_phi; // inverse overdispersion&#39;s parameter real beta; // coefficient on traps real alpha; // intercept on the building vector[J] mu; // varying intercept of the building real&lt;lower = 0&gt; sigma_mu; // sd of the varying intercept vector[K] zeta; // vector of coefficient of building vars } transformed parameters { real phi = inv(inv_phi); // get the original phi } model { /*If you define the linear predictor outside of the likelihood, you must specify it as the first object of the model block.*/ // Likelihood vector[N] eta = mu[building_idx] + // Loop over buildings beta * traps + log_sq_foot; // complaints ~ neg_binomial_2_log(eta, phi); target += neg_binomial_2_log_lpmf(complaints | eta, phi); // Varying intercept on the building // mu ~ normal(alpha + building_data * zeta, sigma_mu); target += normal_lpdf(mu | alpha + building_data * zeta, sigma_mu); // Priors on alpha, beta and inv_phi // alpha ~ normal(log(4), 1); // beta ~ normal(-0.25, 1); // inv_phi ~ normal(0, 1); target += normal_lpdf(alpha| log(4), 1) + normal_lpdf(beta | -0.25, 1) + normal_lpdf(inv_phi | 0, 1); // Priors on the coefficients of buildings // zeta ~ normal(0, 1); // sigma_mu ~ normal(0, 1); target += normal_lpdf(zeta | 0, 1) + normal_lpdf(sigma_mu | 0, 1); } generated quantities { int y_rep[N]; for (n in 1:N) { // Define linear predictor into the loop as a temporary real number real eta_rep = mu[building_idx[n]] + beta * traps[n] + log_sq_foot[n]; y_rep[n] = neg_binomial_2_log_safe_rng(eta_rep, phi); } } We declared in the data block the structure of the building-level matrix and we added in the parameters block the parameters \\(\\mu\\) and \\(\\sigma_{mu}\\) of the distribution of the intercepts and the vector “zeta” of the coefficients of the building-level predictors. We also declared in the model block the contribution to the likelihood of the building-level information. Moreover, we added the term mu[building_idx] both in the model and generated quantities blocks, which tells to Stan to create a vector eta (linear predictor) with a different intercept for each building, that are identified by their id number (declared as [building_idx]) in the code. Now let’s fit the model to the data # Compile the model hier_neg_bin_comp &lt;- stan_model( file = &quot;stan_programs/hier_multiple_neg_bin.stan&quot; ) # Sampling from the posterior fitted_model_NB_hier &lt;- sampling( hier_neg_bin_comp, data = stan_dat_hier, warmup = 1000L, iter = 2000L, chains = 4L, seed = mcmc_seed ) What are all the warnings from Stan? What are divergent transitions? If the Markov chains were not able to explore some regions of the posterior distribution, Stan will return some warnings which tell us that the trajectories of the Markov chains diverged and could not explore some sets of the posterior. When the chains were not able to explore all the regions of the posterior it is not recommended to use such model to do inference. In such situations, we should evaluate the patological behaviors of the algorithm and try to reparametrize the model in order to help the posterior surface exploration. We can diagnose the problems faced by the chains during the sampling, by looking when and in which regions of the posterior the chains diverged. We can inspect this issues using bayesplot R package, the same we used to do posterior predictive checks. Let’s first examine the summaries of the parameters of the model. # Extract the fit of the model post_NB_hier &lt;- rstan::extract(fitted_model_NB_hier) # Print posterior summaries of the most interest parameters print( fitted_model_NB_hier, pars = c( &quot;alpha&quot;, &quot;beta&quot;, &quot;phi&quot;, &quot;mu&quot;, &quot;sigma_mu&quot; ) ) #&gt; Inference for Stan model: hier_multiple_neg_bin. #&gt; 4 chains, each with iter=2000; warmup=1000; thin=1; #&gt; post-warmup draws per chain=1000, total post-warmup draws=4000. #&gt; #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat #&gt; alpha 1.26 0.02 0.43 0.40 1.00 1.27 1.54 2.09 548 1.01 #&gt; beta -0.23 0.00 0.06 -0.35 -0.27 -0.23 -0.19 -0.11 560 1.01 #&gt; phi 1.58 0.01 0.35 1.01 1.34 1.53 1.78 2.40 572 1.01 #&gt; mu[1] 1.27 0.02 0.55 0.16 0.93 1.27 1.63 2.36 598 1.01 #&gt; mu[2] 1.23 0.02 0.53 0.20 0.91 1.22 1.57 2.28 615 1.00 #&gt; mu[3] 1.43 0.02 0.47 0.49 1.12 1.43 1.73 2.39 683 1.00 #&gt; mu[4] 1.46 0.02 0.48 0.50 1.15 1.46 1.76 2.40 618 1.01 #&gt; mu[5] 1.08 0.02 0.43 0.21 0.81 1.09 1.38 1.93 621 1.01 #&gt; mu[6] 1.16 0.02 0.48 0.19 0.86 1.19 1.48 2.07 621 1.01 #&gt; mu[7] 1.48 0.02 0.52 0.42 1.15 1.47 1.83 2.48 572 1.01 #&gt; mu[8] 1.25 0.02 0.42 0.46 0.97 1.24 1.53 2.11 786 1.00 #&gt; mu[9] 1.40 0.02 0.56 0.24 1.04 1.43 1.76 2.49 628 1.01 #&gt; mu[10] 0.86 0.01 0.37 0.15 0.62 0.85 1.10 1.64 721 1.00 #&gt; sigma_mu 0.27 0.01 0.17 0.07 0.15 0.24 0.36 0.69 453 1.01 #&gt; #&gt; Samples were drawn using NUTS(diag_e) at Mon Sep 17 20:13:19 2018. #&gt; For each parameter, n_eff is a crude measure of effective sample size, #&gt; and Rhat is the potential scale reduction factor on split chains (at #&gt; convergence, Rhat=1). The effective sample size (column n_eff) and the \\(\\widehat{R}\\) (column Rhat) are useful indicators of the goodness of the sampling. Effective sample size. When the algorithm run a chain to sample from the posterior, every sample at each iteration can depend from the sample drawn from the previous iteration one. If the samples were totally indepedent, the error in exploring the posterior would be negligible as the square of the number of iteration increases. The effective sample size represents the number of samples from the posterior that are actually independent. Thus, high number of effective sample size means that the Markov Chain has less dependence from one state to the other and it is able to better move around the surface of the posterior. \\(\\widehat{R}\\). The potential scale reduction statistics split-\\(\\widehat{R}\\) measures the ratio of the average variances of draws within each chain to the variance of pooled draws across chains. It is an indicator of the equilibrium of the chains. If all the chains are at equilibrium, then \\(\\widehat{R}\\) will converge to \\(1\\). If chains did not converge to a common distribution, then \\(\\widehat{R}\\) will be greated then \\(1\\). While \\(\\widehat{R}\\) is only slightly greater than \\(1\\) for not all the parameters, low values of the effective sample sizes mean that a lot of drawns from the posterior were dependent from each other. We can look at the traceplot the see if the divergences form a pattern. # use as.array to keep the markov chains separate for trace plots mcmc_trace( as.array( fitted_model_NB_hier, pars = &#39;sigma_mu&#39; ), np = nuts_params(fitted_model_NB_hier), window = c(500,1000) ) The divergent parameters are highlighted with the red bars underneath the traceplots. For such samples, the sampler got stucked at the same parameter value of \\(\\sigma_\\mu\\). We can plot the bivariate distribution of one of the varying intecepts vs \\(\\sigma_\\mu\\) (here we use the logarithm of \\(\\sigma_\\mu\\) the better visualize the distribution). # assign to object so we can compare to another plot later scatter_with_divs &lt;- mcmc_scatter( as.array(fitted_model_NB_hier), pars = c( &quot;mu[4]&quot;, &#39;sigma_mu&#39; ), transform = list(&#39;sigma_mu&#39; = &quot;log&quot;), np = nuts_params(fitted_model_NB_hier) ) scatter_with_divs By looking at the plot above, it seems that divergent transitions mostly occured at the bottom of the distributions. Let’s now look at the prior bivariate distribution to try to understand why divergent transitions were mostly concentrated in the same region of the posterior. # Simulate from the prior distributions ------------------------------ # Number of simulations and vector of the parameters N_sims &lt;- 1000 log_sigma &lt;- rep(NA, N_sims) mu &lt;- rep(NA, N_sims) # Draw from the prior for (j in 1:N_sims) { log_sigma[j] &lt;- rnorm(1, mean = 0, sd = 1) mu[j] &lt;- rnorm(1, mean = 0, sd = exp(log_sigma[j])) } # Bind the vector of draws draws &lt;- cbind( &quot;mu&quot; = mu, &quot;log(sigma_mu)&quot; = log_sigma ) # Plot the bivariate prior distributions mcmc_scatter(draws) As we can see, the bivariate prior looks like a funnel as long as both \\(\\mu\\) and \\(\\sigma_{\\mu}\\) approach \\(0\\). If the data were informative, we wouldn’t expect such a shape of the posterior. Given that our data are very noisy, it is very likely that the posterior will look similar to the prior. Indeed, the divergent transitions mostly occured in the steepest region of the posterior, where the funnel becomes very narrow. Such geometry of the posterior are very difficult to explore for the sampler. Here’s another way to look at the divergent transitions for each pair of \\(\\sigma_{\\mu}\\) and \\(\\mu_{b}\\). parcoord_with_divs &lt;- mcmc_parcoord( as.array( fitted_model_NB_hier, pars = c(&quot;sigma_mu&quot;, &quot;mu&quot;) ), np = nuts_params(fitted_model_NB_hier) ) parcoord_with_divs How can we deal with these issues? We will see later that a simple reparametrization of the model can highly help the sampler to fully explore the surface of the posterior distribution. 3.2.3 Non-centered parametrization We can fix these problems by reparametrizing the distribution of \\(mu\\), the one that causes the divergent transitions of the chains. In particular, we can use the non-centered parametrization. Given that the distribution of \\(mu\\) has been specified as follows: \\[\\begin{equation} \\mu_{b} \\sim N \\left( \\alpha + \\text{building_data} \\zeta, \\sigma_{\\mu} \\right) \\end{equation}\\] we can multiply \\(\\sigma_{\\mu}\\) by an auxiliary variable as: \\[\\begin{equation} \\sigma_{\\mu} = \\sigma_{\\mu} * \\mu_{raw} \\end{equation}\\] This auxiliary variable \\(\\mu_{raw}\\) decouples the depence between the density of \\(\\mu\\) and \\(\\sigma_{mu}\\) without changing the distribution of \\(\\mu_{b}\\). We can modifiy the Stan program as follows: // Hierarchical multiple Negative Binomial model functions { /* * Alternative to neg_binomial_2_log_rng() that * avoids potential numerical problems during warmup */ int neg_binomial_2_log_safe_rng(real eta, real phi) { real gamma_rate = gamma_rng(phi, phi / exp(eta)); if (gamma_rate &gt;= exp(20.79)) return -9; return poisson_rng(gamma_rate); } } data { int&lt;lower = 1&gt; N; int&lt;lower = 0&gt; complaints[N]; vector&lt;lower = 0&gt;[N] traps; vector[N] log_sq_foot; // Declare the hierarchical part int&lt;lower = 1&gt; J; // number of building int&lt;lower = 1&gt; K; // number of building variables matrix[J, K] building_data; // matrix of building data int&lt;lower = 1, upper = J&gt; building_idx[N]; // id of the building } parameters { real&lt;lower = 0&gt; inv_phi; // inverse overdispersion&#39;s parameter real beta; // coefficient on traps real alpha; // intercept on the building vector[J] mu_raw; // auxiliary parameter real&lt;lower = 0&gt; sigma_mu; // sd of the varying intercept vector[K] zeta; // vector of coefficient of building vars } transformed parameters { // get the original phi real phi = inv(inv_phi); // get the original parameter of the varying intercept vector[J] mu = alpha + building_data * zeta + sigma_mu * mu_raw; } model { /*If you define the linear predictor outside of the likelihood, you must specify it as the first object of the model block.*/ // Likelihood vector[N] eta = mu[building_idx] + // Loop over buildings beta * traps + log_sq_foot; // complaints ~ neg_binomial_2_log(eta, phi); target += neg_binomial_2_log_lpmf(complaints | eta, phi); // Priors on alpha, beta and inv_phi // alpha ~ normal(log(4), 1); // beta ~ normal(-0.25, 1); // inv_phi ~ normal(0, 1); target += normal_lpdf(alpha| log(4), 1) + normal_lpdf(beta | -0.25, 1) + normal_lpdf(inv_phi | 0, 1); // Priors on the coefficients of buildings // zeta ~ normal(0, 1); // sigma_mu ~ normal(0, 1); target += normal_lpdf(zeta | 0, 1) + normal_lpdf(mu_raw| 0, 1) + normal_lpdf(sigma_mu | 0, 1); } generated quantities { int y_rep[N]; for (n in 1:N) { // Define linear predictor into the loop as a temporary real number real eta_rep = mu[building_idx[n]] + beta * traps[n] + log_sq_foot[n]; y_rep[n] = neg_binomial_2_log_safe_rng(eta_rep, phi); } } We now fit the model and look at the effective sample size. # Compile the model hier_neg_bin_ncp_comp &lt;- stan_model( file = &quot;stan_programs/hier_multiple_neg_bin_ncp.stan&quot; ) # Sampling from the posterior fitted_model_NB_hier_ncp &lt;- sampling( hier_neg_bin_ncp_comp, data = stan_dat_hier, warmup = 1000L, iter = 2000L, chains = 4L, # 1 chain for a quick sampling seed = mcmc_seed ) print( fitted_model_NB_hier_ncp, pars = c(&#39;sigma_mu&#39;,&#39;beta&#39;,&#39;alpha&#39;,&#39;phi&#39;,&#39;mu&#39;) ) #&gt; Inference for Stan model: hier_multiple_neg_bin_ncp. #&gt; 4 chains, each with iter=2000; warmup=1000; thin=1; #&gt; post-warmup draws per chain=1000, total post-warmup draws=4000. #&gt; #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat #&gt; sigma_mu 0.24 0.01 0.18 0.01 0.11 0.21 0.33 0.68 1173 1 #&gt; beta -0.23 0.00 0.06 -0.35 -0.27 -0.23 -0.19 -0.11 2644 1 #&gt; alpha 1.27 0.01 0.43 0.42 0.99 1.27 1.56 2.10 2620 1 #&gt; phi 1.58 0.01 0.36 1.00 1.33 1.54 1.80 2.38 4000 1 #&gt; mu[1] 1.30 0.01 0.54 0.20 0.96 1.31 1.65 2.39 2665 1 #&gt; mu[2] 1.25 0.01 0.52 0.19 0.92 1.26 1.58 2.28 2726 1 #&gt; mu[3] 1.41 0.01 0.48 0.48 1.08 1.41 1.73 2.37 2929 1 #&gt; mu[4] 1.45 0.01 0.48 0.52 1.13 1.45 1.77 2.39 2908 1 #&gt; mu[5] 1.08 0.01 0.41 0.26 0.82 1.08 1.35 1.88 3223 1 #&gt; mu[6] 1.19 0.01 0.48 0.24 0.88 1.19 1.51 2.12 2626 1 #&gt; mu[7] 1.47 0.01 0.51 0.45 1.13 1.47 1.82 2.47 2859 1 #&gt; mu[8] 1.26 0.01 0.42 0.45 0.98 1.25 1.54 2.09 3377 1 #&gt; mu[9] 1.43 0.01 0.56 0.28 1.07 1.44 1.81 2.51 2845 1 #&gt; mu[10] 0.86 0.01 0.37 0.15 0.63 0.86 1.10 1.62 4000 1 #&gt; #&gt; Samples were drawn using NUTS(diag_e) at Mon Sep 17 20:14:04 2018. #&gt; For each parameter, n_eff is a crude measure of effective sample size, #&gt; and Rhat is the potential scale reduction factor on split chains (at #&gt; convergence, Rhat=1). The number of the effective sample size is greater, meaning that the sampler had less difficult during the exploration of the posterior. Let’s compare the posterior of models with centered and non-centered parametrization. scatter_no_divs &lt;- mcmc_scatter( as.array(fitted_model_NB_hier_ncp), pars = c( &quot;mu[4]&quot;, &#39;sigma_mu&#39; ), transform = list(&#39;sigma_mu&#39; = &quot;log&quot;), np = nuts_params(fitted_model_NB_hier_ncp) ) bayesplot_grid( scatter_with_divs, scatter_no_divs, grid_args = list(ncol = 2), ylim = c(-11, 1) ) parcoord_no_divs &lt;- mcmc_parcoord( as.array( fitted_model_NB_hier_ncp, pars = c(&quot;sigma_mu&quot;, &quot;mu&quot;) ), np = nuts_params(fitted_model_NB_hier_ncp) ) bayesplot_grid( parcoord_with_divs, parcoord_no_divs, ylim = c(-3, 3) ) The sampler explore the posterior of the parameter \\(\\mu_{raw}\\), which has a more isotropic geometry. In such a way, it was able to recover also the region of the posterior not previously explored. Now let’s extract the posterior predictive values and do posterior predictive checks. # Get posterior predictive values post_hier_NB_ncp &lt;- rstan::extract( fitted_model_NB_hier_ncp, pars = c(&#39;y_rep&#39;,&#39;inv_phi&#39;) ) y_rep &lt;- as.matrix( fitted_model_NB_hier_ncp, pars = &quot;y_rep&quot; ) ppc_dens_overlay( stan_dat_hier$complaints, y_rep[1:200,] ) The posterior predicted replicates better resemble the distribution of the observed data. Let’s compare the average number of complaints across buldings, ppc_stat_grouped( y = stan_dat_hier$complaints, yrep = y_rep, group = pest_data$building_id, stat = &#39;mean&#39;, binwidth = 0.5 ) the standard deviations, ppc_stat_grouped( y = stan_dat_hier$complaints, yrep = y_rep, group = pest_data$building_id, stat = &#39;sd&#39;, binwidth = 0.5 ) and the proportion of zeros. ppc_stat_grouped( y = stan_dat_hier$complaints, yrep = y_rep, group = pest_data$building_id, stat = &#39;prop_zero_fun&#39;, binwidth = 0.05 ) We can see that the model tends to overestimate the proportion of zero complaints for building “47”. It is possible that we have very noisy data for building “47” and we cannot estimate precisely the distribution of the outcome variable for that building. Thus, the model tends to be conservative and shrinks the posterior predictive values to the common estimate of a general building. "],
["day4.html", "Day 4 Hierarchical/Multilevel modeling (part 2) 4.1 Varying intercept and varying slopes 4.2 Time varying effects and structured priors 4.3 Use the model 4.4 Exercises", " Day 4 Hierarchical/Multilevel modeling (part 2) 4.1 Varying intercept and varying slopes We retrieved more data and thus we have more number of time points for each buildings we are considering. We can then add complexity to our model allowing for varying slopes (different effect of the number of traps for each building). The model with varying intercepts can be formalize as follows: \\[ \\text{complaints}_{b,t} \\sim \\text{Neg-Binomial}(\\lambda_{b,t}, \\phi) \\\\ \\lambda_{b,t} = \\exp{(\\eta_{b,t})}\\\\ \\eta_{b,t} = \\mu_b + \\kappa_b \\, \\texttt{traps}_{b,t} + \\text{log_sq_foot}_b \\\\ \\mu_b \\sim \\text{Normal}(\\alpha + \\texttt{building_data} \\, \\zeta, \\sigma_{\\mu}) \\\\ \\kappa_b \\sim \\text{Normal}(\\beta + \\texttt{building_data} \\, \\gamma, \\sigma_{\\kappa}) \\] Now let’s load the new dataset. pest_data_longer &lt;- readRDS( here::here( &quot;data/pest_data_longer_stan_dat.RDS&quot; ) ) We will fit the non-centered parametrization version of the model with varying intercepts and slopes with. Here the code of the Stan program // Hierarchical NB model with varying intercepts and slopes functions { /* * Alternative to neg_binomial_2_log_rng() that * avoids potential numerical problems during warmup */ int neg_binomial_2_log_safe_rng(real eta, real phi) { real gamma_rate = gamma_rng(phi, phi / exp(eta)); if (gamma_rate &gt;= exp(20.79)) return -9; return poisson_rng(gamma_rate); } } data { int&lt;lower=1&gt; N; // number of observations int&lt;lower=0&gt; complaints[N]; // number of complaints vector&lt;lower=0&gt;[N] traps; // number of traps vector[N] log_sq_foot; // vector of exposure (offset) // building-level data int&lt;lower=1&gt; K; // number of building-level covs int&lt;lower=1&gt; J; // number of building int&lt;lower=1, upper=J&gt; building_idx[N];// id of the building matrix[J,K] building_data; // building-level matrix } parameters { real&lt;lower=0&gt; inv_phi; // inverse of the parameter phi // Non centered parameters for varying intercepts vector[J] mu_raw; // auxiliary parameter real&lt;lower=0&gt; sigma_mu; // sd of buildings-specific intercepts real alpha; // &#39;global&#39; intercept for buildings vector[K] zeta; // coefficients on building-level predictors // Non centered parameters for varying slopes vector[J] kappa_raw; // auxiliary parameter real&lt;lower=0&gt; sigma_kappa; // sd of buildings-specific slopes real beta; // &#39;global&#39; slope on traps variable vector[K] gamma; // coefficients on building-level predictors } transformed parameters { real phi = inv(inv_phi); // original parameter phi // Original parameters mu and kappa vector[J] mu = alpha + building_data * zeta + sigma_mu * mu_raw; vector[J] kappa = beta + building_data * gamma + sigma_kappa * kappa_raw; } model { // Declare linear predictor Linear predictor vector[N] eta = mu[building_idx] + kappa[building_idx] .* traps + log_sq_foot; // Prior on inv_phi target += normal_lpdf(inv_phi | 0, 1) + // Prior on varying slopes parameters normal_lpdf(kappa_raw | 0, 1) + normal_lpdf(sigma_kappa | 0, 1) + normal_lpdf(beta | -0.25, 1) + normal_lpdf(gamma | 0, 1) + // Prior on varying intercepts parameters normal_lpdf(mu_raw | 0, 1) + normal_lpdf(sigma_mu | 0, 1) + normal_lpdf(alpha | log(4), 1) + normal_lpdf(zeta | 0, 1); // Likelihood target += neg_binomial_2_log_lpmf(complaints | eta, phi); // The symbol &quot;.*&quot; is element-wise multiplication to multiply // the slope of each building for the number of traps of that building } generated quantities { // Declare replicated data int y_rep[N]; for (n in 1:N) { real eta_n = mu[building_idx[n]] + kappa[building_idx[n]] * traps[n] + log_sq_foot[n]; y_rep[n] = neg_binomial_2_log_safe_rng(eta_n, phi); } } Fit the model to data and extract the posterior draws needed for our posterior predictive checks. # Compile the model comp_model_NB_hier_slopes &lt;- stan_model( &#39;stan_programs/hier_multiple_neg_bin_ncp_var_slopes.stan&#39; ) # Sampling from the posterior fitted_model_NB_hier_slopes &lt;- sampling( comp_model_NB_hier_slopes, data = pest_data_longer, warmup = 1000L, iter = 2000L, chains = 4, control = list( adapt_delta = 0.99, # Increase the step of the chains max_treedepth = 15 ), seed = mcmc_seed ) To see if the model infers building-to-building differences in, we can plot a histogram of our marginal posterior distribution for sigma_kappa. mcmc_hist( as.matrix( fitted_model_NB_hier_slopes, pars = &quot;sigma_kappa&quot; ), binwidth = 0.005 ) print( fitted_model_NB_hier_slopes, pars = c(&#39;kappa&#39;,&#39;beta&#39;,&#39;alpha&#39;,&#39;phi&#39;,&#39;sigma_mu&#39;,&#39;sigma_kappa&#39;,&#39;mu&#39;) ) #&gt; Inference for Stan model: hier_multiple_neg_bin_ncp_var_slopes. #&gt; 4 chains, each with iter=2000; warmup=1000; thin=1; #&gt; post-warmup draws per chain=1000, total post-warmup draws=4000. #&gt; #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat #&gt; kappa[1] -0.02 0.00 0.08 -0.14 -0.07 -0.03 0.03 0.16 1273 1.00 #&gt; kappa[2] -0.42 0.00 0.10 -0.64 -0.48 -0.41 -0.35 -0.24 2034 1.00 #&gt; kappa[3] -0.59 0.00 0.10 -0.79 -0.65 -0.58 -0.52 -0.39 4000 1.00 #&gt; kappa[4] -0.22 0.00 0.07 -0.37 -0.26 -0.22 -0.18 -0.08 4000 1.00 #&gt; kappa[5] -0.60 0.00 0.09 -0.78 -0.66 -0.60 -0.54 -0.42 4000 1.00 #&gt; kappa[6] -0.44 0.00 0.11 -0.67 -0.50 -0.43 -0.37 -0.24 4000 1.00 #&gt; kappa[7] -0.31 0.00 0.07 -0.44 -0.36 -0.31 -0.26 -0.18 4000 1.00 #&gt; kappa[8] -0.23 0.00 0.15 -0.56 -0.33 -0.23 -0.13 0.05 4000 1.00 #&gt; kappa[9] 0.08 0.00 0.06 -0.03 0.04 0.08 0.12 0.20 4000 1.00 #&gt; kappa[10] -0.72 0.00 0.16 -1.02 -0.83 -0.73 -0.63 -0.39 1711 1.00 #&gt; beta -0.35 0.00 0.06 -0.47 -0.38 -0.35 -0.31 -0.23 2999 1.00 #&gt; alpha 1.41 0.01 0.32 0.77 1.21 1.41 1.62 2.01 2841 1.00 #&gt; phi 1.61 0.00 0.19 1.27 1.48 1.60 1.73 2.03 4000 1.00 #&gt; sigma_mu 0.50 0.02 0.40 0.02 0.18 0.40 0.73 1.48 601 1.01 #&gt; sigma_kappa 0.13 0.00 0.09 0.03 0.07 0.11 0.16 0.37 643 1.01 #&gt; mu[1] 0.27 0.02 0.73 -1.44 -0.10 0.37 0.76 1.46 1273 1.00 #&gt; mu[2] 1.66 0.01 0.54 0.71 1.28 1.62 1.99 2.85 1972 1.00 #&gt; mu[3] 2.13 0.01 0.32 1.51 1.91 2.12 2.34 2.78 4000 1.00 #&gt; mu[4] 1.48 0.01 0.52 0.44 1.15 1.48 1.80 2.57 4000 1.00 #&gt; mu[5] 2.40 0.01 0.42 1.60 2.11 2.39 2.68 3.21 4000 1.00 #&gt; mu[6] 1.91 0.01 0.40 1.21 1.65 1.88 2.13 2.82 4000 1.00 #&gt; mu[7] 2.68 0.00 0.26 2.19 2.50 2.68 2.85 3.20 4000 1.00 #&gt; mu[8] -0.50 0.02 0.97 -2.31 -1.13 -0.53 0.08 1.60 4000 1.00 #&gt; mu[9] 0.21 0.01 0.58 -0.96 -0.17 0.21 0.59 1.34 4000 1.00 #&gt; mu[10] 1.83 0.03 1.10 -0.66 1.21 1.97 2.58 3.63 1289 1.00 #&gt; #&gt; Samples were drawn using NUTS(diag_e) at Mon Sep 17 20:18:00 2018. #&gt; For each parameter, n_eff is a crude measure of effective sample size, #&gt; and Rhat is the potential scale reduction factor on split chains (at #&gt; convergence, Rhat=1). mcmc_hist( as.matrix( fitted_model_NB_hier_slopes, pars = &quot;beta&quot; ), binwidth = 0.005 ) While the model can’t specifically rule out zero from the posterior, it does have mass at small non-zero numbers, so we should leave in the hierarchy over \\(\\texttt{kappa}\\). Plotting the marginal data density again, we can see the model still looks well calibrated. y_rep &lt;- as.matrix( fitted_model_NB_hier_slopes, pars = &quot;y_rep&quot; ) ppc_dens_overlay( y = pest_data_longer$complaints, yrep = y_rep[1:200,] ) 4.2 Time varying effects and structured priors We haven’t still inspect the trend of complaints over the time. We can check if there is any pattern by comparing the observed and posterior predictive average number of complaints over different months of the year. select_vec &lt;- which(pest_data_longer$mo_idx %in% 1:12) ppc_stat_grouped( y = pest_data_longer$complaints[select_vec], yrep = y_rep[,select_vec], group = pest_data_longer$mo_idx[select_vec], stat = &#39;mean&#39; ) + xlim(0, 11) Looking at the plot above, it seems that the average number of complaints increases over the time. Our model was not able to capture this feature of the data and it tend to overestimate the average number of complaints for many months of the year. We can increase complexity in our model by adding a log-additive monthly effect to capture trend over time with an Autoregressive (AR) model. We add into our model the term \\(\\texttt{mo}_t\\), \\[ \\eta_{b,t} = \\mu_b + \\kappa_b \\, \\texttt{traps}_{b,t} + \\texttt{mo}_t + \\text{log_sq_foot}_b \\] The change in the number of complaints over the time can be influenced by several factors. It is possible that more roaches are present during the summer as well as there is more roach control in the same season. It is plausible to think that maybe residents are more vigilant after the first sighting of roaches in the building, leading to an increase in the number of complaints. This can be a motivation for using an autoregressive prior for our monthly effects. With such model we are evaluating the possibility that the number of complaints in a month is related to the number of complaints in the previous month. The model s as follows: \\[ \\texttt{mo}_t \\sim \\text{Normal}(\\rho \\, \\texttt{mo}_{t-1}, \\sigma_\\texttt{mo}) \\\\ \\equiv \\\\ \\texttt{mo}_t = \\rho \\, \\texttt{mo}_{t-1} +\\epsilon_t , \\quad \\epsilon_t \\sim \\text{Normal}(0, \\sigma_\\texttt{mo}) \\\\ \\quad \\rho \\in [-1,1] \\] This equation says that the monthly effect in month \\(t\\) is directly related to the last month’s monthly effect. Given the description of the process above, it seems like there could be either positive or negative associations between the months, but there should be a bit more weight placed on positive \\(\\rho\\)s, so we’ll put an informative prior that pushes the parameter \\(\\rho\\) towards 0.5. Because Stan doesn’t implement any densities that have support on \\([-1,1]\\), we must use a variable transformation of a raw variable defined on \\([0,1]\\) before having the density on \\(\\rho\\) in \\([-1,1]\\), that is: \\[ \\rho_{\\text{raw}} \\in [0, 1] \\\\ \\rho = 2 \\times \\rho_{\\text{raw}} - 1 \\] In such a way, we can put a prior on \\(\\rho_{raw}\\) that pushes the estimate of \\(\\rho\\) toward \\(0.5\\). Since we are working in a situation where the distribution of \\(mo_{t}\\) is conditional on \\(mo_{t-1}\\), the prior on \\(mo_{t}\\) should follow the same logic. But what kind of prior should we use for the first month, i.e. \\(mo_{1}\\)? For this first observation we need to find its marginal distribution. We can exploit the stationary nature of AR model, that says that for all \\(t\\): \\[ E \\left( mo_{t} \\right) = E \\left( mo_{t -1 } \\right) \\\\ Var \\left( mo_{t} \\right) = Var \\left( mo_{t -1 } \\right) \\] Hence, the marginal distribution of \\(mo_{t}\\) will be equal to the marginal distribution of \\(m_{t - 1}\\). First we derive the marginal variance of \\(\\texttt{mo}_{t}\\). \\[ \\text{Var}(\\texttt{mo}_t) = \\text{Var}(\\rho \\texttt{mo}_{t-1} + \\epsilon_t) \\\\ \\text{Var}(\\texttt{mo}_t) = \\text{Var}(\\rho \\texttt{mo}_{t-1}) + \\text{Var}(\\epsilon_t) \\] The equality in the second line holds because of the independece between of \\(\\epsilon_t\\) and \\(\\epsilon_{t-1})\\). Then, using the fact that \\(Var(cX) = c^2Var(X)\\) for a constant \\(c\\) and that, by stationarity, \\(\\textrm{Var}(\\texttt{mo}_{t-1}) = \\textrm{Var}(\\texttt{mo}_{t})\\), we get: \\[ \\text{Var}(\\texttt{mo}_t)= \\rho^2 \\text{Var}( \\texttt{mo}_{t}) + \\sigma_\\texttt{mo}^2 \\\\ \\text{Var}(\\texttt{mo}_t) = \\frac{\\sigma_\\texttt{mo}^2}{1 - \\rho^2} \\] For the mean of \\(\\texttt{mo}_t\\): \\[ \\mathbb{E}(\\texttt{mo}_t) = \\mathbb{E}(\\rho \\, \\texttt{mo}_{t-1} + \\epsilon_t) \\\\ \\mathbb{E}(\\texttt{mo}_t) = \\mathbb{E}(\\rho \\, \\texttt{mo}_{t-1}) + \\mathbb{E}(\\epsilon_t) \\\\ \\] Since \\(\\mathbb{E}(\\epsilon_t) = 0\\) by assumption we have \\[ \\mathbb{E}(\\texttt{mo}_t) = \\mathbb{E}(\\rho \\, \\texttt{mo}_{t-1}) + 0\\\\ \\mathbb{E}(\\texttt{mo}_t) = \\rho \\, \\mathbb{E}(\\texttt{mo}_{t}) \\\\ \\mathbb{E}(\\texttt{mo}_t) - \\rho \\mathbb{E}(\\texttt{mo}_t) = 0 \\\\ \\mathbb{E}(\\texttt{mo}_t) = 0/(1 - \\rho) \\] which for \\(\\rho \\neq 1\\) yields \\(\\mathbb{E}(\\texttt{mo}_{t}) = 0\\). We thus get the marginal distribution for \\(\\texttt{mo}_{t}\\), which we will use for \\(\\texttt{mo}_1\\). The AR model for \\(mo_{1}\\) can be specified as follows: \\[ \\texttt{mo}_1 \\sim \\text{Normal}\\left(0, \\frac{\\sigma_\\texttt{mo}}{\\sqrt{1 - \\rho^2}}\\right) \\] Thus, the prior will have the following distribution: \\[ \\texttt{mo}_t \\sim \\text{Normal}\\left(\\rho \\, \\texttt{mo}_{t-1}, \\sigma_\\texttt{mo}\\right) \\forall t &gt; 1 \\] The Stan program of the last model is coded as follows: // Hierarchical NB model with varying intercepts and slopes and // month effect functions { /* * Alternative to neg_binomial_2_log_rng() that * avoids potential numerical problems during warmup */ int neg_binomial_2_log_safe_rng(real eta, real phi) { real gamma_rate = gamma_rng(phi, phi / exp(eta)); if (gamma_rate &gt;= exp(20.79)) return -9; return poisson_rng(gamma_rate); } } data { int&lt;lower = 1&gt; N; int&lt;lower = 0&gt; complaints[N]; vector&lt;lower = 0&gt;[N] traps; // &#39;exposure&#39; vector[N] log_sq_foot; // building-level data int&lt;lower = 1&gt; K; int&lt;lower = 1&gt; J; int&lt;lower = 1, upper = J&gt; building_idx[N]; matrix[J,K] building_data; // month info int&lt;lower = 1&gt; M; int&lt;lower = 1, upper = M&gt; mo_idx[N]; } parameters { real&lt;lower = 0&gt; inv_phi; // 1/phi (easier to think about prior for 1/phi instead of phi) // Varying intercept for the buildings vector[J] mu_raw; // N(0,1) params for non-centered param of building-specific intercepts real&lt;lower = 0&gt; sigma_mu; // sd of buildings-specific intercepts real alpha; // &#39;global&#39; intercept vector[K] zeta; // coefficients on building-level predictors in model for mu // Varying slopes for the buildings vector[J] kappa_raw; // N(0,1) params for non-centered param of building-specific slopes real&lt;lower = 0&gt; sigma_kappa; // sd of buildings-specific slopes real beta; // &#39;global&#39; slope on traps variable vector[K] gamma; // coefficients on building-level predictors in model for kappa // month-specific parameters real&lt;lower = 0,upper = 1&gt; rho_raw; // used to construct rho, the AR(1) coefficient vector[M] mo_raw; real&lt;lower = 0&gt; sigma_mo; } transformed parameters { real phi = inv(inv_phi); // non-centered parameterization of building-specific intercepts and slopes vector[J] mu = alpha + building_data * zeta + sigma_mu * mu_raw; vector[J] kappa = beta + building_data * gamma + sigma_kappa * kappa_raw; // AR(1) process priors real rho = 2.0 * rho_raw - 1.0; vector[M] mo = sigma_mo * mo_raw; mo[1] /= sqrt(1 - rho^2); // mo[1] = mo[1]/sqrt(1-rho^2) // loop over the rest of the mo vector to add in the dependence on previous month for(m in 2:M) { mo[m] += rho * mo[m - 1]; } } model { // Likelihood vector[N] eta = mu[building_idx] + kappa[building_idx] .* traps + mo[mo_idx] + log_sq_foot; target += neg_binomial_2_log_lpmf(complaints | eta, phi); // Priors target += normal_lpdf(inv_phi | 0, 1) + // Priors on non-centered slopes normal_lpdf(kappa_raw | 0, 1) + normal_lpdf(sigma_kappa | 0, 1) + normal_lpdf(beta | -0.25, 1) + normal_lpdf(gamma | 0, 1) + // Priors on non-centered intercepts normal_lpdf(mu_raw | 0, 1) + normal_lpdf(sigma_mu | 0, 1) + normal_lpdf(alpha | log(4), 1) + normal_lpdf(zeta | 0, 1) + // Priors on non-centered months beta_lpdf(rho_raw | 10, 5) + normal_lpdf(mo_raw | 0, 1) + normal_lpdf(sigma_mo | 0, 1); // Alternative formulation // inv_phi ~ normal(0, 1); // // kappa_raw ~ normal(0,1) ; // sigma_kappa ~ normal(0, 1); // beta ~ normal(-0.25, 1); // gamma ~ normal(0, 1); // // mu_raw ~ normal(0,1) ; // sigma_mu ~ normal(0, 1); // alpha ~ normal(log(4), 1); // zeta ~ normal(0, 1); // // rho_raw ~ beta(10, 5); // mo_raw ~ normal(0, 1); // sigma_mo ~ normal(0, 1); // // complaints ~ neg_binomial_2_log(mu[building_idx] + // kappa[building_idx] .* traps + // mo[mo_idx] + // log_sq_foot, // phi); } generated quantities { int y_rep[N]; for (n in 1:N) { real eta_n = mu[building_idx[n]] + kappa[building_idx[n]] * traps[n] + mo[mo_idx[n]] + log_sq_foot[n]; y_rep[n] = neg_binomial_2_log_safe_rng(eta_n, phi); } } Let’s compile the model comp_model_NB_hier_mos &lt;- stan_model( &#39;stan_programs/hier_multiple_neg_bin_ncp_var_slopes_mon.stan&#39; ) and run the algorithm to sample from the posterior. fitted_model_NB_hier_mos &lt;- sampling( comp_model_NB_hier_mos, data = pest_data_longer, warmup = 1000L, iter = 2000L, chains = 4, control = list( adapt_delta = 0.95, max_treedepth = 15 ), seed = mcmc_seed ) Now we can print the parameters of the model. print( fitted_model_NB_hier_mos, pars = c( &#39;kappa&#39;, &#39;beta&#39;, &#39;alpha&#39;, &#39;phi&#39;, &#39;sigma_mu&#39;, &#39;sigma_kappa&#39;, &#39;mu&#39;, &quot;zeta&quot;, &quot;mo&quot;, &quot;sigma_mo&quot; ) ) #&gt; Inference for Stan model: hier_multiple_neg_bin_ncp_var_slopes_mon. #&gt; 4 chains, each with iter=2000; warmup=1000; thin=1; #&gt; post-warmup draws per chain=1000, total post-warmup draws=4000. #&gt; #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat #&gt; kappa[1] -0.12 0.00 0.05 -0.22 -0.15 -0.12 -0.09 -0.04 2178 1.00 #&gt; kappa[2] -0.28 0.00 0.07 -0.42 -0.33 -0.28 -0.24 -0.16 4000 1.00 #&gt; kappa[3] -0.26 0.00 0.07 -0.40 -0.31 -0.26 -0.22 -0.13 4000 1.00 #&gt; kappa[4] -0.18 0.00 0.05 -0.28 -0.22 -0.19 -0.15 -0.07 2037 1.00 #&gt; kappa[5] -0.34 0.00 0.07 -0.47 -0.38 -0.34 -0.29 -0.19 4000 1.00 #&gt; kappa[6] -0.25 0.00 0.06 -0.38 -0.28 -0.24 -0.20 -0.12 4000 1.00 #&gt; kappa[7] -0.06 0.00 0.04 -0.14 -0.09 -0.06 -0.03 0.02 4000 1.00 #&gt; kappa[8] -0.41 0.00 0.12 -0.65 -0.48 -0.40 -0.34 -0.20 2280 1.00 #&gt; kappa[9] -0.04 0.00 0.04 -0.12 -0.07 -0.04 -0.01 0.05 4000 1.00 #&gt; kappa[10] -0.49 0.00 0.09 -0.66 -0.54 -0.48 -0.43 -0.33 4000 1.00 #&gt; beta -0.24 0.00 0.04 -0.34 -0.27 -0.24 -0.22 -0.16 2376 1.00 #&gt; alpha 0.81 0.01 0.45 -0.06 0.51 0.80 1.10 1.71 1266 1.00 #&gt; phi 8.78 0.03 1.94 5.69 7.40 8.52 9.88 13.44 4000 1.00 #&gt; sigma_mu 0.30 0.01 0.24 0.01 0.12 0.26 0.43 0.89 1240 1.00 #&gt; sigma_kappa 0.09 0.00 0.05 0.01 0.05 0.07 0.11 0.22 1190 1.00 #&gt; mu[1] 0.88 0.02 0.61 -0.26 0.46 0.86 1.27 2.13 1344 1.01 #&gt; mu[2] 0.53 0.01 0.52 -0.50 0.18 0.53 0.87 1.56 1455 1.00 #&gt; mu[3] 0.79 0.01 0.46 -0.11 0.50 0.80 1.09 1.70 1226 1.00 #&gt; mu[4] 0.78 0.02 0.57 -0.36 0.42 0.79 1.17 1.86 1352 1.00 #&gt; mu[5] 0.81 0.01 0.50 -0.19 0.49 0.81 1.15 1.78 1434 1.00 #&gt; mu[6] 0.84 0.01 0.47 -0.08 0.54 0.84 1.15 1.81 1292 1.00 #&gt; mu[7] 1.54 0.01 0.44 0.67 1.25 1.54 1.81 2.42 1124 1.01 #&gt; mu[8] 0.29 0.02 0.81 -1.24 -0.26 0.27 0.81 1.96 2006 1.00 #&gt; mu[9] 1.00 0.01 0.59 -0.15 0.62 1.00 1.40 2.19 1858 1.00 #&gt; mu[10] 0.51 0.02 0.74 -0.92 0.01 0.51 1.01 1.97 2449 1.00 #&gt; zeta[1] -0.13 0.01 0.43 -0.99 -0.41 -0.14 0.14 0.75 2836 1.00 #&gt; zeta[2] 0.37 0.01 0.35 -0.34 0.14 0.38 0.61 1.03 2595 1.00 #&gt; zeta[3] 0.08 0.01 0.50 -0.95 -0.24 0.08 0.41 1.07 3242 1.00 #&gt; zeta[4] -0.23 0.01 0.33 -0.87 -0.46 -0.24 -0.01 0.46 2816 1.00 #&gt; mo[1] -2.23 0.02 0.59 -3.51 -2.60 -2.20 -1.84 -1.18 1523 1.00 #&gt; mo[2] -1.60 0.01 0.51 -2.66 -1.93 -1.59 -1.26 -0.62 1546 1.00 #&gt; mo[3] -1.77 0.01 0.53 -2.87 -2.11 -1.77 -1.42 -0.74 1604 1.00 #&gt; mo[4] -1.45 0.01 0.50 -2.48 -1.77 -1.45 -1.13 -0.46 1472 1.00 #&gt; mo[5] -1.68 0.01 0.52 -2.72 -2.02 -1.67 -1.33 -0.68 1602 1.00 #&gt; mo[6] -1.43 0.01 0.50 -2.46 -1.75 -1.43 -1.10 -0.46 1418 1.00 #&gt; mo[7] -1.55 0.01 0.53 -2.62 -1.89 -1.54 -1.21 -0.56 1361 1.00 #&gt; mo[8] -0.89 0.01 0.49 -1.89 -1.20 -0.89 -0.58 0.05 1363 1.00 #&gt; mo[9] -0.27 0.01 0.46 -1.19 -0.57 -0.26 0.03 0.60 1320 1.00 #&gt; mo[10] -0.79 0.01 0.48 -1.75 -1.10 -0.78 -0.47 0.14 1296 1.00 #&gt; mo[11] -0.96 0.01 0.49 -1.97 -1.28 -0.94 -0.64 -0.03 1305 1.00 #&gt; mo[12] 0.13 0.01 0.46 -0.79 -0.15 0.14 0.43 1.01 1261 1.01 #&gt; mo[13] 0.45 0.01 0.45 -0.47 0.16 0.46 0.73 1.30 1206 1.01 #&gt; mo[14] 0.96 0.01 0.44 0.06 0.68 0.97 1.25 1.82 1205 1.01 #&gt; mo[15] -0.06 0.01 0.46 -1.00 -0.34 -0.05 0.23 0.84 1217 1.01 #&gt; mo[16] 0.22 0.01 0.45 -0.69 -0.06 0.22 0.52 1.10 1205 1.00 #&gt; mo[17] 0.32 0.01 0.45 -0.62 0.03 0.32 0.60 1.20 1198 1.00 #&gt; mo[18] 0.14 0.01 0.46 -0.76 -0.16 0.14 0.44 1.01 1200 1.00 #&gt; mo[19] 0.64 0.01 0.44 -0.26 0.36 0.64 0.94 1.50 1165 1.01 #&gt; mo[20] 0.33 0.01 0.45 -0.58 0.04 0.33 0.62 1.21 1197 1.00 #&gt; mo[21] 0.78 0.01 0.45 -0.14 0.49 0.78 1.06 1.64 1183 1.01 #&gt; mo[22] 1.20 0.01 0.44 0.33 0.92 1.20 1.48 2.07 1186 1.01 #&gt; mo[23] 0.99 0.01 0.44 0.12 0.71 0.99 1.27 1.83 1191 1.01 #&gt; mo[24] 0.74 0.01 0.45 -0.16 0.46 0.75 1.03 1.61 1173 1.01 #&gt; mo[25] 0.67 0.01 0.45 -0.22 0.38 0.68 0.97 1.54 1213 1.00 #&gt; mo[26] 0.81 0.01 0.44 -0.09 0.53 0.81 1.09 1.69 1216 1.00 #&gt; mo[27] 0.87 0.01 0.45 -0.03 0.59 0.88 1.15 1.75 1216 1.00 #&gt; mo[28] 0.94 0.01 0.44 0.03 0.66 0.94 1.23 1.80 1225 1.00 #&gt; mo[29] 0.15 0.01 0.46 -0.77 -0.14 0.16 0.45 1.06 1250 1.00 #&gt; mo[30] 0.17 0.01 0.46 -0.76 -0.12 0.17 0.48 1.07 1191 1.00 #&gt; mo[31] 0.81 0.01 0.45 -0.07 0.53 0.82 1.09 1.67 1194 1.00 #&gt; mo[32] 1.03 0.01 0.45 0.12 0.75 1.03 1.30 1.90 1155 1.01 #&gt; mo[33] 1.45 0.01 0.44 0.57 1.17 1.45 1.73 2.34 1162 1.01 #&gt; mo[34] 0.47 0.01 0.45 -0.46 0.20 0.48 0.76 1.36 1221 1.00 #&gt; mo[35] 0.26 0.01 0.46 -0.66 -0.03 0.26 0.55 1.16 1192 1.01 #&gt; mo[36] 0.66 0.01 0.45 -0.23 0.37 0.66 0.95 1.54 1177 1.01 #&gt; sigma_mo 0.58 0.00 0.10 0.42 0.51 0.57 0.64 0.80 1009 1.00 #&gt; #&gt; Samples were drawn using NUTS(diag_e) at Mon Sep 17 20:20:47 2018. #&gt; For each parameter, n_eff is a crude measure of effective sample size, #&gt; and Rhat is the potential scale reduction factor on split chains (at #&gt; convergence, Rhat=1). In the interest of brevity, we won’t go on expanding the model, though we certainly could. What other information would help us understand the data generating process better? What other aspects of the data generating process might we want to capture that we’re not capturing now? As usual, we run through our posterior predictive checks. y_rep &lt;- as.matrix( fitted_model_NB_hier_mos, pars = &quot;y_rep&quot; ) ppc_dens_overlay( y = pest_data_longer$complaints, yrep = y_rep[1:200,] ) select_vec &lt;- which(pest_data_longer$mo_idx %in% 1:12) ppc_stat_grouped( y = pest_data_longer$complaints[select_vec], yrep = y_rep[,select_vec], group = pest_data_longer$mo_idx[select_vec], stat = &#39;mean&#39; ) As we can see, our monthly varying intercept has captured a monthly pattern across all the buildings. We can also compare the prior and posterior for the autoregressive parameter to see how much we’ve learned. Here are two different ways of comparing the prior and posterior visually: # 1) compare draws from prior and draws from posterior rho_draws &lt;- cbind( 2 * rbeta(4000, 10, 5) - 1, # draw from prior as.matrix( fitted_model_NB_hier_mos, pars = &quot;rho&quot; ) ) colnames(rho_draws) &lt;- c(&quot;prior&quot;, &quot;posterior&quot;) mcmc_hist( rho_draws, freq = FALSE, binwidth = 0.025, facet_args = list(nrow = 2) ) + xlim(-1, 1) # 2) overlay prior density curve on posterior draws gen_rho_prior &lt;- function(x) { alpha &lt;- 10; beta &lt;- 5 a &lt;- -1; c &lt;- 1 lp &lt;- (alpha - 1) * log(x - a) + (beta - 1) * log(c - x) - (alpha + beta - 1) * log(c - a) - lbeta(alpha, beta) return(exp(lp)) } mcmc_hist( as.matrix( fitted_model_NB_hier_mos, pars = &quot;rho&quot; ), freq = FALSE, binwidth = 0.01 ) + overlay_function(fun = gen_rho_prior) + xlim(-1,1) print( fitted_model_NB_hier_mos, pars = c(&#39;rho&#39;,&#39;sigma_mu&#39;,&#39;sigma_kappa&#39;,&#39;gamma&#39;) ) #&gt; Inference for Stan model: hier_multiple_neg_bin_ncp_var_slopes_mon. #&gt; 4 chains, each with iter=2000; warmup=1000; thin=1; #&gt; post-warmup draws per chain=1000, total post-warmup draws=4000. #&gt; #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat #&gt; rho 0.77 0.00 0.08 0.58 0.72 0.78 0.83 0.91 1501 1 #&gt; sigma_mu 0.30 0.01 0.24 0.01 0.12 0.26 0.43 0.89 1240 1 #&gt; sigma_kappa 0.09 0.00 0.05 0.01 0.05 0.07 0.11 0.22 1190 1 #&gt; gamma[1] -0.18 0.00 0.10 -0.38 -0.25 -0.18 -0.12 0.01 1705 1 #&gt; gamma[2] 0.12 0.00 0.07 -0.03 0.07 0.11 0.16 0.27 1663 1 #&gt; gamma[3] 0.11 0.00 0.14 -0.17 0.02 0.10 0.19 0.39 2117 1 #&gt; gamma[4] -0.01 0.00 0.06 -0.14 -0.04 0.00 0.04 0.12 1919 1 #&gt; #&gt; Samples were drawn using NUTS(diag_e) at Mon Sep 17 20:20:47 2018. #&gt; For each parameter, n_eff is a crude measure of effective sample size, #&gt; and Rhat is the potential scale reduction factor on split chains (at #&gt; convergence, Rhat=1). ppc_intervals( y = pest_data_longer$complaints, yrep = y_rep, x = pest_data_longer$traps ) + labs( x = &quot;Number of traps&quot;, y = &quot;Number of complaints&quot; ) It looks as if our model finally generates a reasonable posterior predictive distribution for all numbers of traps, and appropriately captures the tails of the data generating process. 4.3 Use the model We can now use our model to help the company on the decision of the optimal number of traps to put in each building. We will make predictions for \\(6\\) months forward. Our revenue model needs to know how much revenue is lost due to the complaints. We know that the company for every \\(10\\) complaints will call an exterminator agency that will cost around 100 euros, nearly 10 euros per complaint. We now prepare the data for our model. We need to add in list to pass to the Stan program a vector with the number of traps for which we want to evaluate the number of complaints and the lost revenue for each complaints. # Number of hypothetical traps N_hypo_traps &lt;- 21L hypo_traps &lt;- seq(from = 0, to = 20, by = 1) # List with data to pass to Stan pest_data_longer[[&quot;N_hypo_traps&quot;]] &lt;- N_hypo_traps pest_data_longer[[&quot;hypo_traps&quot;]] &lt;- hypo_traps pest_data_longer[[&quot;lost_rev&quot;]] &lt;- 10 The Stan program has been coded as follows: // Predictions using hierarchical NB model with varying intercepts // and slopes and month effect functions { /* * Alternative to neg_binomial_2_log_rng() that * avoids potential numerical problems during warmup */ int neg_binomial_2_log_safe_rng(real eta, real phi) { real gamma_rate = gamma_rng(phi, phi / exp(eta)); if (gamma_rate &gt;= exp(20.79)) return -9; return poisson_rng(gamma_rate); } } data { int&lt;lower = 1&gt; N; int&lt;lower = 0&gt; complaints[N]; vector&lt;lower = 0&gt;[N] traps; // &#39;exposure&#39; vector[N] log_sq_foot; // building-level data int&lt;lower = 1&gt; K; // number of building-level predictors int&lt;lower = 1&gt; J; // number of buildings int&lt;lower = 1, upper = J&gt; building_idx[N]; // building id matrix[J, K] building_data; // building-level matrix // month info int&lt;lower = 1&gt; M; int&lt;lower = 1, upper = M&gt; mo_idx[N]; // To use in the generated quantities block int&lt;lower = 1&gt; M_forward; vector[J] log_sq_foot_pred; // Number of traps used to predict number of complaints int N_hypo_traps; int hypo_traps[N_hypo_traps]; // Lost revenue for one complaint real lost_rev; } parameters { real&lt;lower = 0&gt; inv_phi; // inverse of phi // Varying intercept for the buildings vector[J] mu_raw; // auxiliary parameter real&lt;lower = 0&gt; sigma_mu;// sd of buildings-specific intercepts real alpha; // &#39;global&#39; intercept vector[K] zeta; // coefficients on building-level predictors // Varying slopes for the buildings vector[J] kappa_raw; // auxiliary parameter real&lt;lower = 0&gt; sigma_kappa; // sd of buildings-specific slopes real beta; // &#39;global&#39; slope on traps variable vector[K] gamma; // coefficients on building-level predictors // month-specific parameters real&lt;lower = 0,upper = 1&gt; rho_raw; // used to construct rho vector[M] mo_raw; real&lt;lower = 0&gt; sigma_mo; } transformed parameters { real phi = inv(inv_phi); // original phi // non-centered parameterization of building-specific vector[J] mu = alpha + building_data * zeta + sigma_mu * mu_raw; vector[J] kappa = beta + building_data * gamma + sigma_kappa * kappa_raw; // AR(1) process priors real rho = 2.0 * rho_raw - 1.0; vector[M] mo = sigma_mo * mo_raw; mo[1] /= sqrt(1 - rho^2); // mo[1] = mo[1]/sqrt(1-rho^2) // add in the dependence on previous month for(m in 2:M) { mo[m] += rho * mo[m - 1]; } } model { // Likelihood vector[N] eta = mu[building_idx] + kappa[building_idx] .* traps + mo[mo_idx] + log_sq_foot; target += neg_binomial_2_log_lpmf(complaints | eta, phi); // Priors target += normal_lpdf(inv_phi | 0, 1) + // Priors on non-centered slopes normal_lpdf(kappa_raw | 0, 1) + normal_lpdf(sigma_kappa | 0, 1) + normal_lpdf(beta | -0.25, 1) + normal_lpdf(gamma | 0, 1) + // Priors on non-centered intercepts normal_lpdf(mu_raw | 0, 1) + normal_lpdf(sigma_mu | 0, 1) + normal_lpdf(alpha | log(4), 1) + normal_lpdf(zeta | 0, 1) + // Priors on non-centered months beta_lpdf(rho_raw | 10, 5) + normal_lpdf(mo_raw | 0, 1) + normal_lpdf(sigma_mo | 0, 1); } generated quantities { /* we&#39;ll predict number of complaints and revenue lost for each building at each hypothetical number of traps for M_forward months in the future*/ int y_pred[J, N_hypo_traps]; matrix[J, N_hypo_traps] rev_pred; for (j in 1:J) { // loop over buildings for (i in 1:N_hypo_traps) { // loop over hypothetical traps int y_pred_by_month[M_forward]; // monthly predictions vector[M_forward] mo_forward; // number of month forward // first future month depends on last observed month mo_forward[1] = normal_rng(rho * mo[M], sigma_mo); for (m in 2:M_forward) { mo_forward[m] = normal_rng(rho * mo_forward[m-1], sigma_mo); } for (m in 1:M_forward) { real eta = mu[j] + kappa[j] * hypo_traps[i] + mo_forward[m] + log_sq_foot_pred[j]; y_pred_by_month[m] = neg_binomial_2_log_safe_rng(eta, phi); } // Sum the number of complaints by month for each number of // traps in each building y_pred[j, i] = sum(y_pred_by_month); /* We were were told every 10 complaints has additional exterminator cost of $100, so $10 lose per complaint.*/ rev_pred[j,i] = y_pred[j,i] * (-lost_rev); } } } We fit the model and run the sampler. # Compile the model comp_model_rev &lt;- stan_model( file = &quot;stan_programs/hier_multiple_neg_bin_ncp_var_slopes_mon_predict.stan&quot; ) # Sampling from the posterio fitted_model_rev &lt;- sampling( object = comp_model_rev, data = pest_data_longer, warmup = 1000L, iter = 2000L, chains = 4L, control = list( adapt_delta = 0.95, max_treedepth = 15 ), seed = mcmc_seed ) In our analysis, the cost of installing a bait station plays a key role and we need to understand the cost associated with maintaining each bait station over the year. We know that the cost associated to the yearly maintaintion of a bait station is about 20 euros. We must also account for the cost of the labor of maintaining the bait stations, which is needed every two months. If there are less than \\(5\\) traps in the building, the cost for the maintaintion is about \\(20\\) euros every two months. If the number of traps is greater than \\(5\\), then the cost is about \\(30\\) euros. Let’s now create the vector of costs. N_traps &lt;- 20L # Number of hypothetical traps costs &lt;- 10 * (0:N_traps) # Trap costs N_months_forward &lt;- 12L # Number of months to predict for N_months_labor &lt;- N_months_forward/2 # Number of labor months # Hourly price of maintainance labor rate_low &lt;- 20 rate_high &lt;- 30 # Total costs costs &lt;- costs + (0:N_traps &lt; 5 &amp; 0:N_traps &gt; 0) * (N_months_labor * rate_low) + (0:N_traps &gt;= 5 &amp; 0:N_traps &lt; 10) * (N_months_labor * (rate_low + 1 * rate_high)) + (0:N_traps &gt;= 10 &amp; 0:N_traps &lt; 15) * (N_months_labor * (rate_low + 2 * rate_high)) + (0:N_traps &gt;= 15) * (N_months_labor * (rate_low + 3 * rate_high)) Now we plot the curves that related the number of traps and the associated money loss with relative uncertainty intervals. # extract as a list for convenience below samps_rev &lt;- rstan::extract(fitted_model_rev) # total profit: revenues minus costs tot_profit &lt;- sweep( samps_rev$rev_pred, 3, STATS = costs, FUN = &#39;-&#39; ) # Median profit median_profit &lt;- t(apply(tot_profit, c(2, 3), median)) # lower and upper ends of 50% central interval lower_profit &lt;- t(apply(tot_profit, c(2, 3), quantile, 0.25)) upper_profit &lt;- t(apply(tot_profit, c(2, 3), quantile, 0.75)) profit_df &lt;- data_frame( profit = as.vector(median_profit), lower = as.vector(lower_profit), upper = as.vector(upper_profit), traps = rep(0:N_traps, times = N_buildings), building = rep(1:N_buildings, each = N_traps + 1) ) ggplot( data = profit_df, mapping = aes( x = traps, y = profit ) ) + geom_ribbon( mapping = aes( ymin = lower, ymax = upper ), fill = &quot;grey70&quot; ) + geom_line() + facet_wrap( ~ building, scales = &#39;free_y&#39;, ncol = 2 ) + theme_bw() The optimal number of traps differs for each building. 4.4 Exercises How would we build a revenue for a new building? Hint: generating a new intercept and a new slope from the posterior predictive distribution Let’s say our utility function is revenue. If we wanted to maximize expected revenue, we can take expectations at each station count for each building, and choose the trap numbers that maximizes expected revenue. This will be called a maximum revenue strategy. How can we generate the distribution of portfolio revenue (i.e. the sum of revenue across all the buildings) under the maximum revenue strategy from the draws of rev_pred we already have? "],
["day5.html", "Day 5 Model comparison", " Day 5 Model comparison How can we compare more fitted model to choose which model to use to support the decision making process? We want to assess how the models predict outcome values with new values. The most common approach involves training a model on a set of data (training set) and validating the model on other set of data (validation set) on which we evaluate the predictive performance. In most situations we want to decide which model to use before collecting new data that can be used for model validation. In such situations, we can make some assumptions on the performance of the models if we would collect new data. We can make some assumptions on predictive density, which measures how surprisingly the data are compare with our model. Basically, when we have a new data point, the model tells us how the new value is reasonable given model assumptions. If we have new data, we can evaluate the average reasonability of the data given our model. We can get some measure performance even if we don’t have a new data. We can define a measure of predictive accuracy for the \\(n\\) observed data, which we will call Expected Log Pointwise Predictive Density (ELPD): \\[ \\sum_{i = 1}^{n} \\int p_{t} \\left( \\tilde{y_{i}} \\right) p \\left( \\tilde{y_{i}} \\vert y_{i} \\right) d \\tilde{y_{i}} \\] where \\(p_{t} \\left( \\tilde{y_{i}} \\right)\\) is the distribution of the true data generating process and \\(p \\left( \\tilde{y_{i}} | y_{i} \\right)\\) is the log predictive density. Obviously, \\(p_{t} \\left( \\tilde{y_{i}} \\right)\\) is unknown, so we need to approximate it using WAIC or cross-validation. On the observed data, we can compute the Log Pointwise Predictive Density (LPD) as follows: \\[ \\sum_{i = 1}^{n} \\log p \\left( y_{i} \\vert y \\right) = \\\\ \\sum_{i = 1}^{n} \\log \\int p \\left( y_{i} \\vert \\theta \\right) p \\left( \\theta \\vert y \\right) d \\theta \\] In practice, the LPD can be computed by evaluating the its expectation over the posterior draws of \\(\\theta\\). Denoting with \\(S\\) the samples drawn from the posterior: \\[ \\widehat{LPD} = \\sum_{i = 1}^{n} \\log \\left( \\frac{1}{S} \\sum_{s = 1}^{S} p \\left( y_{i} \\vert \\theta_{s} \\right)\\right) \\] Of course, the LPD on the observed data will overconfident, because it is computed on the same set of observations used to train the model. The face the issue of overconfidence with prediction on the training set, we can use the Leave-One-Out Cross-Validation (LOO-CV). Basically it consists of training the model on all the observed data but one. The left observation is used as validation set. The operation is repeated for all the data points and then the individual log posterior density is summed across the observations. The Bayesian LOO-CV can be formalized as follows: \\[ ELPD_{loo} = \\sum_{i = 1}^{n} \\log p \\left( y_{i} \\vert y_{i - 1} \\right) \\] where \\[ p \\left( y_{i} \\vert y_{i - 1} \\right) = \\int p \\left( y_{i} \\vert \\theta \\right) p \\left( \\theta \\vert y_{i - 1} \\right) \\] We can compare the difference of ELPD between two models on the data points to check how the models measure the plausibility of each data points and how the models predict the data points (maybe a model is able to better predict some data points than the other). The problem is that if the model is slow or if we have a lot of data points, fitting \\(n\\) models is not feasible. How can we compute LOO-CV without fitting \\(n\\) models? We can approximate LOO-CV performance as follows: fit the model once and then use Pareto Smoothed Importance Sampling (PSIS-LOO) A weight is associated to each observation and it measures the importance of the observation in the computation of the posterior of the parameters of the model. A Pareto distribution is then fitted to the distribution of weights. The largest weights with order statistics with Pareto distribution are replaced. We assume that the posterior is not highly sensitive to leaving out a single data point. How would we know if such an assumption holds? Maybe there are some observations which are relevant for the model. Based on the estimate of the shape parameter (\\(k\\)) of Pareto distribution we can know if such assumption holds. For larger values of \\(k\\) the loo package gives some warnings, telling us that some observations are very important for the model and the posterior may be sensitive if leaving out those observations. If warnings are thrown out by the package, it means that PSIS is not reliable in such situation. For the problematic observations we can compute the LPD exactly by fitting the model to all the other data and evaluate the elpd on the particular observations. Models are then compared by looking at their ELPD. The one with higher value is the one that should be preferred in terms of predictive performance. For further details on the use of LOO-CV we remind the reader to the study of Vehtari et al. (2017) . We will show how perform Bayesian model comparison using loo R package. The demonstration will be implemented on some fake data in we will use a logistic regression to model the prediction of an hypothetical event. First we define the parameters of the data generating process. # Consider a model with 2 continuous variables, one binary variable # and interaction between the binary and one of the two continuous # variables # Number of observations n &lt;- 5000L # Number of coefficients k &lt;- 5L # Define the vector of parameters beta_vector &lt;- c( -log(2.5), # intercept log(0.8), # x1 (cont) log(1.1), # x2 (cont) log(2.2), # x3 (binary) log(1.5) # x1*x3 ) # Alternative code to generate data with R # # Generate matrix of covariates # des_mat &lt;- data_frame( # x1 = rnorm(n = n, mean = 0, sd = 1), # x2 = rnorm(n = n, mean = 0, sd = 1), # x3 = rbinom(n = n, size = 1, prob = 0.4) # ) %&gt;% # model.matrix( # ~ x1 + x2 + x3 + x1:x3, # data = . # ) # # # Linear predictor # eta &lt;- des_mat %*% beta_vector # # # Transform on probability scale # prob &lt;- 1/(1 + exp(-eta)) # # # Get the outcome variable # y &lt;- rbinom(n = n, size = 1, prob = prob) # # # Get the proportion of events # table(y) Here’s the Stan program to generate fake data from the prior predictive distribution. // Logistic regression Data Generating Process data{ int&lt;lower = 1&gt; N; // number of observations int&lt;lower = 1&gt; K; // number of coefficients vector[K] beta; // coefficients } parameters{ } model { } generated quantities { // Declare simulated variables vector[N] x1; vector[N] x2; int&lt;lower = 0, upper = 1&gt; x3[N]; int&lt;lower = 0, upper = 1&gt; y[N]; // Generate parameters value from the prior predictive distribution // (try a t_student) real alpha = student_t_rng(7, beta[1], 0.1); real beta_x1 = student_t_rng(7, beta[2], 0.1); real beta_x2 = student_t_rng(7, beta[3], 0.1); real beta_x3 = student_t_rng(7, beta[4], 0.1); real beta_x1_x3 = student_t_rng(7, beta[5], 0.1); // Declare linear predictor real eta; // Generate simulated values of the outcome for (n in 1:N) { // Generate covariates x1[n] = normal_rng(0, 1); x2[n] = normal_rng(0, 1); x3[n] = bernoulli_rng(0.4); // Linear predictor eta = alpha + beta_x1 * x1[n] + beta_x2 * x2[n] + beta_x3 * x3[n] + beta_x1_x3 * x1[n] * x3[n]; y[n] = bernoulli_logit_rng(eta); } } Now let’s compile the model and sample from the prior. # List of data to pass to Stan fake_list &lt;- list( N = n, K = k, beta = beta_vector ) # Compile the data generating process model logit_dgp_comp &lt;- stan_model( file = &quot;stan_programs/loo_logit_dgp.stan&quot; ) # Sample to get fake data samps_logit_dgp &lt;- sampling( object = logit_dgp_comp, data = fake_list, chains = 1L, cores = 1L, iter = 1L, algorithm = &quot;Fixed_param&quot;, seed = mcmc_seed ) #&gt; #&gt; SAMPLING FOR MODEL &#39;loo_logit_dgp&#39; NOW (CHAIN 1). #&gt; Iteration: 1 / 1 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0 seconds (Warm-up) #&gt; 0 seconds (Sampling) #&gt; 0 seconds (Total) # Get fake data fake_data &lt;- rstan::extract(samps_logit_dgp) # Store fake data into a list fake_data_list &lt;- list( N = n, K = k, beta = beta_vector, x1 = fake_data$x1[1, ], x2 = fake_data$x2[1, ], x3 = fake_data$x3[1, ], y = fake_data$y[1, ] ) We now fit the full model to check if we can recover the parameter values of the data generating process. The Stan program of the full model is as follows: // Logistic regression full model data{ int&lt;lower = 1&gt; N; // number of observations int&lt;lower =1&gt; K; // number of coefficients vector[K] beta; // vector of coefficients vector[N] x1; // continuous predictor vector[N] x2; // continuous predictor vector&lt;lower = 0, upper = 1&gt;[N] x3; // binary predictor int&lt;lower = 0, upper = 1&gt; y[N]; // outcome } parameters { real alpha; real beta_x1; real beta_x2; real beta_x3; real beta_x1_x3; } model { // Linear predictor vector[N] eta; // Prios target += student_t_lpdf(alpha | 7, beta[1], 1) + student_t_lpdf(beta_x1 | 7, beta[2], 1) + student_t_lpdf(beta_x2 | 7, beta[3], 1) + student_t_lpdf(beta_x3 | 7, beta[4], 1) + student_t_lpdf(beta_x1_x3 | 7, beta[5], 1); // Linear predictor eta = alpha + beta_x1 * x1 + beta_x2 * x2 + beta_x3 * x3 + beta_x1_x3 * x1 .* x3; // Likelihood target += bernoulli_logit_lpmf(y | eta); } generated quantities{ vector[N] y_rep; // replicated data vector[N] log_lik; // pointwise log likelihood for LOO-CV for(n in 1:N) { real eta_rep = alpha + beta_x1 * x1[n] + beta_x2 * x2[n] + beta_x3 * x3[n] + beta_x1_x3 * x1[n] * x3[n]; y_rep[n] = bernoulli_logit_rng(eta_rep); log_lik[n] = bernoulli_logit_lpmf(y[n] | eta_rep); } } In the generated quantities we store the log lik vector. It contains the log predictive pointwise density of the observed data, which we will use later to compute the ELPD using the LOO-CV. # Compile the full model full_logit_comp &lt;- stan_model( file = &quot;stan_programs/loo_logit_full_model.stan&quot; ) # Sample from the posterior fitted_full_logit &lt;- sampling( object = full_logit_comp, data = fake_data_list, warmup = 1000L, iter = 2000L, chains = 4L, seed = mcmc_seed ) # Get the posterior of the parameters post_alpha_betas &lt;- as.matrix( fitted_full_logit, pars = c( &quot;alpha&quot;, &quot;beta_x1&quot;, &quot;beta_x2&quot;, &quot;beta_x3&quot;, &quot;beta_x1_x3&quot; ) ) # Get the parameter values of the data generating process true_alpha_beta &lt;- c( fake_data$alpha, fake_data$beta_x1, fake_data$beta_x2, fake_data$beta_x3, fake_data$beta_x1_x3 ) # Plot the true values mcmc_recover_hist( x = post_alpha_betas, true = true_alpha_beta ) The model seems to be able to recover the parameters of the data generating process. We will compare 3 models: A model with only continuous variables A model with continuous variables and the binary variable The full model We will compile the first two models and we will sample from their posteriors. The full model has been already compiled and we have already sampled from its posterior. # Compiles the models logit_1_comp &lt;- stan_model( file = &quot;stan_programs/loo_logit_model_1.stan&quot; ) logit_2_comp &lt;- stan_model( file = &quot;stan_programs/loo_logit_model_2.stan&quot; ) # Sample from their posteriors fitted_logit_1 &lt;- sampling( object = logit_1_comp, data = fake_data_list, warmup = 1000L, iter = 2000L, chains = 4L, seed = mcmc_seed ) fitted_logit_2 &lt;- sampling( object = logit_2_comp, data = fake_data_list, warmup = 1000L, iter = 2000L, chains = 4L, seed = mcmc_seed ) We can now extract the LPD for each model and compute ELPD with LOO-CV with PSIS. # Extract lpd of each model log_lik_1 &lt;- extract_log_lik( stanfit = fitted_logit_1, merge_chains = FALSE ) r_eff_1 &lt;- relative_eff(exp(log_lik_1)) log_lik_2 &lt;- extract_log_lik( stanfit = fitted_logit_2, merge_chains = FALSE ) r_eff_2 &lt;- relative_eff(exp(log_lik_2)) log_lik_full &lt;- extract_log_lik( stanfit = fitted_full_logit, merge_chains = FALSE ) r_eff_full &lt;- relative_eff(exp(log_lik_full)) # Compute ELPD for each model with LOO-CV loo_1 &lt;- loo(log_lik_1, r_eff = r_eff_1) loo_2 &lt;- loo(log_lik_2, r_eff = r_eff_2) loo_full &lt;- loo(log_lik_full, r_eff = r_eff_full) Now we compare the first two models. # Model 1 and 2 compare(loo_1, loo_2) #&gt; elpd_diff se #&gt; 155.7 17.5 As we expected, ELPD is higher in the second model, because we are modeling the outcome with one more predictor which we imposed to be a strong one. Let’s now compare the second and the full model. compare(loo_2, loo_full) #&gt; elpd_diff se #&gt; 13.3 5.3 The full has higher ELPD because we included the interaction term. "],
["references.html", "References", " References "]
]
